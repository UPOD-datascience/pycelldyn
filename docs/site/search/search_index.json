{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the documentation of","text":"<p>A Python package for working with Kite's manufacturing data. </p> <p>Currently, <code>KiPy</code> works with the <code>CLP</code> and <code>XLP</code> data files, but this might be expanded in the future.</p>"},{"location":"setup/","title":"Setup","text":""},{"location":"setup/#installation","title":"Installation","text":"<ol> <li> <p>Create a new environment using either <code>conda</code> (recommended) or <code>venv</code> and activate it</p> </li> <li> <p>Use MSAT's Cookiecutter for Data Science Projects and generate a new project template.</p> </li> <li> <p>Install the <code>KyPi</code> package using <code>pip</code> with the following command:</p> </li> </ol> <pre><code>pip install git+https://github.com/kite-msat-ds/kipy\n</code></pre> <p>Warning</p> <p><code>pip</code> will try to install the required dependencies. However, for some reason (probably because of Kite's firewall) sometimes the SSL certificate can't be confirmed and the installation throws an error:</p> <pre><code>Could not fetch URL https://pypi.python.org/simple/PACKAGENAME/: There was a problem \nconfirming the ssl certificate: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify \nfailed (_ssl.c:777) - skipping\nCould not find a version that satisfies the requirement PACKAGENAME (from versions: XXX)\n</code></pre> <p>In that case, you need to install the missing packages one by one by hand with the following command:</p> <pre><code>pip install PACKAGENAME --trusted-host pypi.python.org --trusted-host files.pythonhosted.org --trusted-host pypi.org\n</code></pre> <p>After all the dependencies have been installed, you will be able to install <code>KiPy</code> with the original command.</p>"},{"location":"setup/#configuring-clpxlp-automatic-update","title":"Configuring CLP/XLP Automatic Update","text":"<p>When working with the CLP/XLP files, we need a copy of them. These are usually hosted online (e.g., <code>Working CLP Data.xlsx</code>, <code>Working XLP Data.xlsx</code>). Normally, you would have to download them to your local machine and point <code>KiPy</code>  to them (see <code>read_excel_file_raw</code>). </p> <p>However, this is a very cumbersome process. To avoid this, Diana has created a Power Automate tool that creates a local copy of these files and updates them everyday (at 7:00/8:00 h, depending on daylight savings time) as long as changes have happened. Configuring this is very easy, just follow these steps:</p> <ol> <li> <p>Go to the following Sharepoint page, where the files are being constantly updated.</p> </li> <li> <p>In the top ribbon, click on <code>Sync</code>. This will create a directory in your computer where these files will be synchronized. </p> <p></p> </li> <li> <p>DONE!</p> </li> </ol> <p>Tip</p> <p>Once you have this files in your local machine, you can copy them to your project directory... or you can use <code>KiPy</code>'s <code>copy_most_recent_data_file</code>, which does exactly that.</p> <p>Warning</p> <p>If you see that your files haven't been updated, it could be because there have been no changes since the last copy was generated. </p> <p>If you are sure that there are new changes and yet the synchronization failed,  please send an email to diana.carreropinto@gilead.com. Please provide as much information as possible,  including the approximate day and time of when the error happened.</p>"},{"location":"api_reference/deviations/","title":"Deviations","text":"<p>These functions are used for identifying and classifying deviations.  These are very useful for process monitoring.</p> <p><code>get_action_limits</code></p> <p>Get the action limits of a given product.</p> <p>Danger</p> <p>If any of the action limits are changed, this function needs  to be updated.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>The product of interest. Possible values are:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <code>param_name</code> <code>str</code> <p>Parameter name. For possible values, see the documents</p> <ul> <li>YESCARTA Column Mapping.docx</li> <li>TECARTUS Column Mapping.docx</li> </ul> required <p>Returns:</p> Type Description <code>lal, ual</code> <p>A tuple with the lower action limit and upper action limit of the given parameter.</p> <p><code>get_control_limits</code></p> <p>Get the control limits for a given product.</p> <p>Danger</p> <p>If any of the control limits are changed, this function needs  to be updated.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>The product of interest. Possible values are:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <code>param_name</code> <code>str</code> <p>Parameter name. For possible values, see the documents</p> <ul> <li>YESCARTA Column Mapping.docx</li> <li>TECARTUS Column Mapping.docx</li> </ul> required <p>Returns:</p> Type Description <code>lcl, ucl</code> <p>A tuple with the lower control limit and upper control limit of the given parameter.</p> <p><code>get_median</code></p> <p>Get the median of the control charts for a given product.</p> <p>Danger</p> <p>If any of the median values are changed, this function needs  to be updated.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>The product of interest. Possible values are:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <code>param_name</code> <code>str</code> <p>Parameter name. For possible values, see the documents * YESCARTA Column Mapping.docx * TECARTUS Column Mapping.docx</p> required <p>Returns:</p> Name Type Description <code>median</code> <code>float</code> <p>Float with the median of the given parameter.</p> <p>Info</p> <p>Values were extracted from the process monitoring control charts generated by PREDICTUM.</p> Source code in <code>kipy\\deviations.py</code> <pre><code>def get_median(product, param_name):\n    \"\"\"`get_median`\n\n    Get the median of the control charts for a given product.\n\n    !!! danger\n\n        If any of the median values are changed, this function needs \n        to be updated.\n\n    Parameters\n    ----------\n    product : str\n        The product of interest. Possible values are:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    param_name : str\n        Parameter name. For possible values, see the documents\n        * YESCARTA Column Mapping.docx\n        * TECARTUS Column Mapping.docx\n\n    Returns\n    -------\n    median : float\n        Float with the median of the given parameter.\n\n        !!! info\n            Values were extracted from the process monitoring control charts\n            generated by PREDICTUM.\n    \"\"\"\n    if (product == 'yescarta') or (product == 'clp'):\n        # Exceptions\n        if param_name == 'neatcell_viability':\n            #TODO\n            median = np.nan\n        elif param_name == 'neatcell_recovery':\n            #TODO\n            median = np.nan\n        elif param_name == 'pbmc_thaw_wash_viability':\n            #TODO\n            median = np.nan\n        # Notice that these values for PBMC are the same as frozen, \n        # since they are considered kinda the same.\n        elif param_name == 'tcell_activation_density_pbmc':\n            #TODO\n            median = np.nan\n        elif param_name == 'tcell_activation_density_frozen':\n            #TODO\n            median = np.nan\n        elif param_name == 'tcell_activation_density_fresh':\n            #TODO\n            median = np.nan\n        elif param_name == 'viability_wash1':\n            #TODO\n            median = np.nan\n        elif param_name == 'transduction_postseeding_density':\n            #TODO\n            median = np.nan\n        elif param_name == 'viability_wash2':\n            #TODO\n            median = np.nan\n        elif param_name == 'tcell_min_expansion_viability':\n            #TODO\n            median = np.nan\n        # These values were updated after after AGILE-CC-01156\n        elif param_name == 'fold_expansion_per_day':\n            #TODO\n            median = np.nan\n        elif param_name == 'recovery_wash3':\n            #TODO\n            median = np.nan\n\n        # OOS\n        elif param_name == 'viability_harvest':\n            #TODO\n            median = np.nan\n        elif param_name == 'transduction_harvest':\n            #TODO\n            median = np.nan\n        elif param_name == 'ifng':\n            #TODO\n            median = np.nan\n        elif param_name == 'cd3_cells_p':\n            #TODO\n            median = np.nan\n        # VCN by ddPCR\n        elif (param_name == 'vcn_bag1') or (param_name == 'vcn_bag2'):\n            #TODO\n            median = np.nan\n        else:\n            raise Exception(f\"Median for YESCARTA parameter {param_name} not defined\")\n\n\n    elif (product == 'tecartus') or (product == 'xlp'):\n        # Exceptions\n        if param_name == 'apheresis_viability':\n            #TODO\n            median = np.nan\n        elif param_name == 'viability_postwash2':\n            #TODO\n            median = np.nan\n        elif param_name == 'postclinimacs_viability':\n            #TODO\n            median = np.nan\n        elif param_name == 'viability_postwash3':\n            #TODO\n            median = np.nan\n        elif param_name == 'excess_tcell_viability_postwash':\n            #TODO\n            median = np.nan\n        elif param_name == 'vcd_seeding_postactivation':\n            #TODO\n            median = np.nan\n        elif param_name == 'viability_postwash4':\n            #TODO\n            median = np.nan\n        elif param_name == 'vcd_seeding_posttransduction':\n            #TODO\n            median = np.nan\n        elif param_name == 'tcell_min_expansion_viability':\n            #TODO\n            median = np.nan\n        elif param_name == 'fold_expansion_per_day':\n            #TODO\n            median = np.nan\n        elif param_name == 'harvest_wash_recovery':\n            #TODO\n            median = np.nan\n        elif param_name == 'vcd_average_final_formulation':\n            #TODO\n            median = np.nan\n\n        # OOS\n        elif param_name == 'viability_harvest':\n            median = 88\n        elif param_name == 'transduction_harvest':\n            median = 72\n        elif param_name == 'ifng':\n            median = 8356\n        elif (param_name == 'vcn_bag1') or (param_name == 'vcn_bag2'):\n            median = 6\n        elif param_name == 'cd3_cells_p':\n            median = 99\n\n        else:\n            raise Exception(f\"Median for TECARTUS parameter {param_name} not defined\")\n\n    else:\n        raise Exception(f\"{product} is not a defined product\")\n\n    return median\n</code></pre> <p><code>get_oos_cols</code></p> <p>Get the name of columns that will be evaluated for OOS.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>The product of interest. Possible values are:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <p>Returns:</p> Name Type Description <code>cols_oos</code> <code>list</code> <p>List of the column names that will be evaluated for OOS.</p> <p><code>get_excursion_cols</code></p> <p>Get the name of columns that will be evaluated for excursions.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>The product of interest. Possible values are:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <p>Returns:</p> Name Type Description <code>cols_excursion</code> <code>list</code> <p>List of the column names that will be evaluated for excursions.</p> <p><code>get_deviation_cols_extras</code></p> <p>Get the name of additional columns that will be evaluated for deviations, but aren't strictly either OOS or excursions.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>The product of interest. Possible values are:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <p>Returns:</p> Name Type Description <code>cols_extra</code> <code>list</code> <p>List of the column names that will be evaluated for deviations that are neither OOS or excursions.</p> <p><code>classify_deviation</code> </p> <p>Calculate if a parameter has a deviation (i.e., either excursion or OOS) or not.</p> <p>Tip</p> <p>Remember deviations are defined based on action limits  (and not on control limits).</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>The product of interest. Possible values are:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <code>df</code> <code>pandas DataFrame</code> <p>Original DataFrame.</p> required <code>param</code> <code>string</code> <p>Parameter name. Possible values are the columns defined in  <code>kipy.get_oos_cols</code> and <code>kipy.get_excursion_cols</code></p> required <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_deviation</code> <code>pandas DataFrame</code> <p>A copy of the input DataFrame with an extra boolean column named <code>param + _deviation</code> that defines if a row was a deviation (True)  or not (False).</p> <p><code>classify_deviation_extra</code></p> <p>Calculate if an extra parameter has a deviation or not.</p> <p>Info</p> <p>These so-called extra parameters are those that are neither an  excursion (related to IPCs) or an OOS (related to CQAs) since they aren't defined as neither. However, they are still asked as part of the process monitoring.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>The product of interest. Possible values are:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <code>df</code> <code>pandas DataFrame</code> <p>Original DataFrame.</p> required <code>param</code> <code>string</code> <p>Parameter name. Possible values are the columns defined in  <code>kipy.get_deviation_cols_extra</code> plus <code>vi_bags</code> and <code>vcn_bags</code>.</p> required <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_deviation</code> <code>pandas DataFrame</code> <p>A copy of the input DataFrame with an extra column named <code>col_name_deviation</code> that defines if a row was a deviation (True)  or not (False).</p> <p><code>get_batches_oos_list</code></p> <p>Obtain a DataFrame with a of batches and a list of their OOS.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame with <code>_oos</code> columns as obtained from functions <code>kipy.classify_deviation</code>, <code>kipy.classify_deviation_extra</code>, and <code>kipy.count_deviations</code>.</p> required <p>verbose : bool     Define if verbose output will be printed (True) or not (False).</p> <p>Returns:</p> Name Type Description <code>df_oos_cases</code> <code>pandas DataFrame</code> <p>DataFrame with the batches with OOS. Each row is a batch. It has the following columns:</p> Parameter name Output DataFrame name <code>batch_id</code> Batch ID <code>production_date_formatted</code> Production date <code>oos_description</code> OOS list <code>status</code> Status <p>Warning</p> <p>Do not change the names of the columns of the output DataFrame. If they are changed, they will also need to be updated in the Power BI dashboard.</p> <p><code>count_deviations</code></p> <p>Add columns that count the number of excursions/OOS for each batch.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>The product of interest. Possible values are:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> <p>Info</p> <p>Currently, there is no distinction between YESCARTA and  TECARTUS. However, we leave this parameter as a place holder in case it is needed in the future.</p> required <code>df</code> <code>pandas DataFrame</code> <p>Original DataFrame.</p> required <code>deviation_type</code> <code>string</code> <p>Deviation type of interest. Possible values are:</p> <ul> <li><code>excursion</code> (related to IPCs)</li> <li><code>oos</code> (related to CQAs)</li> </ul> required <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_deviations_counted</code> <code>pandas DataFrame</code> <p>A copy of the input DataFrame with two extra columns. Their names depend on the <code>deviation_type</code> of interest.</p> <code>deviation_type</code> Added columns <code>excursion</code> * <code>excursion_total</code>: number of excursions of the given batch. * <code>excursion</code>: bool that indicates if the batch had one excursion or more. <code>oos</code> * <code>oos_total</code>: number of OOS of the given batch. * <code>oos</code>: bool that indicates if the batch had one OOS or more. Source code in <code>kipy\\deviations.py</code> <pre><code>def count_deviations(product, df, deviation_type, verbose=True):\n    \"\"\" `count_deviations`\n\n    Add columns that count the number of excursions/OOS for each batch.\n\n    Parameters\n    ----------\n    product : str\n        The product of interest. Possible values are:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n        !!! info\n            Currently, there is no distinction between YESCARTA and \n            TECARTUS. However, we leave this parameter as a place holder\n            in case it is needed in the future.\n\n    df : pandas DataFrame\n        Original DataFrame.\n\n    deviation_type : string\n        Deviation type of interest. Possible values are:\n\n        * `excursion` (related to IPCs)\n        * `oos` (related to CQAs)\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df_deviations_counted : pandas DataFrame\n        A copy of the input DataFrame with two extra columns. Their names\n        depend on the `deviation_type` of interest.\n\n        | `deviation_type` | Added columns                                                              |\n        |------------------|----------------------------------------------------------------------------|\n        | `excursion`      | * `excursion_total`: number of excursions of the given batch.              |\n        |                  | * `excursion`: bool that indicates if the batch had one excursion or more. |\n        | `oos`            | * `oos_total`: number of OOS of the given batch.                           |\n        |                  | * `oos`: bool that indicates if the batch had one OOS or more.             |\n    \"\"\"\n    df_deviations_counted = df.copy(deep=True)\n\n    # Get a list of all the deviation columns.\n    cols_all = list(df.columns)\n    deviation_cols = misc.get_elements_with_substring(cols_all,  ['_' + deviation_type])\n\n    # We remove this `vcn_bags_oos` to avoid counting this OOS twice, \n    # since `vcn_bags_oos` depends on vcn_bag1_oos and vcn_bag2_oos.\n    if 'vcn_bags_oos' in deviation_cols:\n        deviation_cols.remove('vcn_bags_oos')\n\n\n    _df_tmp = df[deviation_cols]\n\n    # Count the number of `deviation_type`s at hand \n    # (e.g., sum of all OOS, sum of all excursions)\n    df_deviations_counted[deviation_type + '_total'] = _df_tmp.sum(axis=1) \n\n    # Define if `deviation_type` was present \n    # (e.g., if there was at least one OOS, the batch is considered OOS)\n    df_deviations_counted[deviation_type] = _df_tmp.any(axis=1)\n\n    return df_deviations_counted\n</code></pre> <p><code>get_deviation_summary</code></p> <p>Get a summary with the number and percentage of batches with deviations (OOS or exception) of a given DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame with <code>_oos</code> columns as obtained from functions <code>kipy.classify_deviation</code>, <code>kipy.classify_deviation_extra</code>, and <code>kipy.count_deviations</code>.</p> required <code>deviation_type</code> <code>str</code> <p>Define which type of deviations will be counted. Possible values are: </p> <ul> <li> <p><code>'oos'</code></p> </li> <li> <p><code>'excursion'</code></p> </li> </ul> required <code>partitioned</code> <code>bool</code> <p>If <code>True</code>, the data will be partitioned into past and present (thus <code>date_end</code> is a mandatory input). If <code>False</code>, the summary will  be provided for all data.</p> <code>False</code> <code>date_end</code> <code>Given by </code> <p>End date as given by <code>kipy.get_date_end</code></p> <code>None</code> <code>offset</code> <code>int</code> <p>Offset for <code>date_start</code>. </p> <p>In case <code>date_end</code> corresponds to a monthly resolution, <code>offset</code> is  in months. </p> <p>In case <code>date_end</code> corresponds to a quarterly resolution, <code>offset</code> is  in quarters.</p> <p>Tip</p> <p>For the current Process Monitoring format, <code>offset</code> should be set to:</p> <ul> <li>For <code>monthly</code>, 6 (i.e., six months)</li> <li>For <code>quarterly</code>, 2 (i.e., six months)</li> </ul> <code>None</code> <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (True) or not (False).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_deviation_summary</code> <code>pandas DataFrame</code> <p>For each OOS/excursion column, the output DataFrame will have a <code>_n</code> and a <code>_p</code> row for number and percentage, respectively.</p> <p>In case <code>partitioned == True</code>, additionally there will be a <code>past_</code> and a <code>present_</code> case.</p>"},{"location":"api_reference/deviations/#kipy.get_action_limits--references","title":"References","text":"<ul> <li> <p>YESCARTA in-process controls (IPCs) are defined in the report \"REP-23526 Control Limits Justification for YESCARTA at TCF04\" (p. 4, Table 1)</p> </li> <li> <p>YESCARTA critical quality attributes (CQAs) are defined in the report \"REP-23526 Control Limits Justification for YESCARTA at TCF04\" (p. 8, Table 3)</p> </li> <li> <p>TECARTUS in-process controls (IPCs) are defined in the report \"REP-43562 Control Limits Justification for the Production of TECARTUS at TCF04\" (p. 5, Table 1).</p> </li> <li> <p>TECARTUS critical quality attributes (CQAs) are defined in the report \"REP-43562 Control Limits Justification for the Production of TECARTUS at TCF04\" (p. 7, Table 3).</p> </li> </ul> Source code in <code>kipy\\deviations.py</code> <pre><code>def get_action_limits(product, param_name):\n    \"\"\"`get_action_limits`\n\n    Get the action limits of a given product.\n\n    !!! danger\n\n        If any of the action limits are changed, this function needs \n        to be updated.\n\n    Parameters\n    ----------\n    product : str\n        The product of interest. Possible values are:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    param_name : str\n        Parameter name. For possible values, see the documents\n\n        * YESCARTA Column Mapping.docx\n        * TECARTUS Column Mapping.docx\n\n\n    Returns\n    -------\n    lal, ual : tuple\n        A tuple with the lower action limit and upper action limit\n        of the given parameter.\n\n    References\n    ----------\n    * YESCARTA in-process controls (IPCs) are defined in the report\n    \"REP-23526 Control Limits Justification for YESCARTA at TCF04\"\n    (p. 4, Table 1)\n\n    * YESCARTA critical quality attributes (CQAs) are defined in the report\n    \"REP-23526 Control Limits Justification for YESCARTA at TCF04\"\n    (p. 8, Table 3)\n\n    * TECARTUS in-process controls (IPCs) are defined in the report\n    \"REP-43562 Control Limits Justification for the Production of TECARTUS at TCF04\"\n    (p. 5, Table 1).\n\n    * TECARTUS critical quality attributes (CQAs) are defined in the report\n    \"REP-43562 Control Limits Justification for the Production of TECARTUS at TCF04\"\n    (p. 7, Table 3).\n    \"\"\"\n\n    if (product == 'yescarta') or (product == 'clp'):\n        # Exceptions\n        if param_name == 'neatcell_viability':\n            lal = 86\n            ual = np.nan\n        elif param_name == 'neatcell_recovery':\n            lal = 16\n            ual = np.nan\n        elif param_name == 'pbmc_thaw_wash_viability':\n            lal = 77\n            ual = np.nan\n        # Notice that these values for PBMC are the same as frozen, \n        # since they are considered kinda the same.\n        elif param_name == 'tcell_activation_density_pbmc':\n            lal = 0.65e6\n            ual = 2.50e6\n        elif param_name == 'tcell_activation_density_frozen':\n            lal = 0.65e6\n            ual = 2.50e6\n        elif param_name == 'tcell_activation_density_fresh':\n            lal = 0.65e6\n            ual = 2.50e6\n        elif param_name == 'viability_wash1':\n            lal = 59\n            ual = np.nan\n        elif param_name == 'transduction_postseeding_density':\n            lal = 0.32e6\n            ual = 0.80e6\n        elif param_name == 'viability_wash2':\n            lal = 68\n            ual = np.nan\n        elif param_name == 'tcell_min_expansion_viability':\n            lal = 77\n            ual = np.nan\n        # These values were updated after after AGILE-CC-01156\n        elif param_name == 'fold_expansion_per_day':\n            lal = 0.5\n            ual = 3.2\n        elif param_name == 'recovery_wash3':\n            lal = 46\n            ual = np.nan\n\n        # OOS\n        elif param_name == 'viability_harvest':\n            lal = 80\n            ual = np.nan\n        elif param_name == 'transduction_harvest':\n            lal = 15\n            ual = np.nan\n        elif param_name == 'ifng':\n            lal = 520\n            ual = np.nan\n        elif param_name == 'cd3_cells_p':\n            lal = 85\n            ual = np.nan\n        # VCN by ddPCR\n        elif (param_name == 'vcn_bag1') or (param_name == 'vcn_bag2'):\n            lal = np.nan\n            ual = 10\n        else:\n            raise Exception(f\"Action limits for YESCARTA parameter {param_name} not defined\")\n\n\n    elif (product == 'tecartus') or (product == 'xlp'):\n        # Exceptions\n        if param_name == 'apheresis_viability':\n            lal = 95\n            ual = np.nan\n        elif param_name == 'viability_postwash2':\n            lal = 89\n            ual = np.nan\n        elif param_name == 'postclinimacs_viability':\n            lal = 86\n            ual = np.nan\n        elif param_name == 'viability_postwash3':\n            lal = 83\n            ual = np.nan\n        elif param_name == 'excess_tcell_viability_postwash':\n            lal = 88\n            ual = np.nan\n        elif param_name == 'vcd_seeding_postactivation':\n            lal = 0.4e6\n            ual = 5.5e6\n        elif param_name == 'viability_postwash4':\n            lal = 56\n            ual = np.nan\n        elif param_name == 'vcd_seeding_posttransduction':\n            lal = 0.3e6\n            ual = 2.1e6\n        elif param_name == 'tcell_min_expansion_viability':\n            lal = 53\n            ual = np.nan\n        elif param_name == 'fold_expansion_per_day':\n            lal = 0.2\n            ual = np.nan\n        elif param_name == 'harvest_wash_recovery':\n            lal = 61\n            ual = np.nan\n        elif param_name == 'vcd_average_final_formulation':\n            lal = 1e6\n            ual = np.nan\n\n        # OOS\n        elif param_name == 'viability_harvest':\n            lal = 72\n            ual = np.nan\n        elif param_name == 'transduction_harvest':\n            lal = 24\n            ual = np.nan\n        elif param_name == 'ifng':\n            lal = 190\n            ual = np.nan\n        elif (param_name == 'vcn_bag1') or (param_name == 'vcn_bag2'):\n            lal = np.nan\n            ual = 10\n        elif param_name == 'cd3_cells_p':\n            lal = 90\n            ual = np.nan\n\n        else:\n            raise Exception(f\"Action limits for TECARTUS parameter {param_name} not defined\")\n\n    else:\n        raise Exception(f\"{product} is not a defined product\")\n\n    return lal, ual\n</code></pre>"},{"location":"api_reference/deviations/#kipy.get_control_limits--references","title":"References","text":"<ul> <li> <p>YESCARTA in-process controls (IPCs) are defined in the report REP-23526 Control Limits Justification for YESCARTA at TCF04 (p. 4, Table 1)</p> </li> <li> <p>YESCARTA critical quality attributes (CQAs) are defined in the report REP-23526 Control Limits Justification for YESCARTA at TCF04 (p. 8, Table 3)</p> </li> <li> <p>TECARTUS in-process controls (IPCs) are defined in the report REP-43562 Control Limits Justification for the Production of TECARTUS at TCF04 (p. 5, Table 1).</p> </li> <li> <p>TECARTUS critical quality attributes (CQAs) are defined in the report REP-43562 Control Limits Justification for the Production of TECARTUS at TCF04 (p. 7, Table 3).</p> </li> </ul> Source code in <code>kipy\\deviations.py</code> <pre><code>def get_control_limits(product, param_name):\n    \"\"\"`get_control_limits`\n\n    Get the control limits for a given product.\n\n    !!! danger\n\n        If any of the control limits are changed, this function needs \n        to be updated.\n\n    Parameters\n    ----------\n    product : str\n        The product of interest. Possible values are:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    param_name : str\n        Parameter name. For possible values, see the documents\n\n        * YESCARTA Column Mapping.docx\n        * TECARTUS Column Mapping.docx\n\n\n    Returns\n    -------\n    lcl, ucl : tuple\n        A tuple with the lower control limit and upper control limit\n        of the given parameter.\n\n    References\n    ----------\n    * YESCARTA in-process controls (IPCs) are defined in the report\n    REP-23526 Control Limits Justification for YESCARTA at TCF04\n    (p. 4, Table 1)\n\n    * YESCARTA critical quality attributes (CQAs) are defined in the report\n    REP-23526 Control Limits Justification for YESCARTA at TCF04\n    (p. 8, Table 3)\n\n    * TECARTUS in-process controls (IPCs) are defined in the report\n    REP-43562 Control Limits Justification for the Production of TECARTUS at TCF04\n    (p. 5, Table 1).\n\n    * TECARTUS critical quality attributes (CQAs) are defined in the report\n    REP-43562 Control Limits Justification for the Production of TECARTUS at TCF04\n    (p. 7, Table 3).\n    \"\"\"\n    if (product == 'yescarta') or (product == 'clp'):\n        # Exceptions\n        if param_name == 'neatcell_viability':\n            lcl = 97\n            ucl = 100\n        elif param_name == 'neatcell_recovery':\n            lcl = 43\n            ucl = 85\n        elif param_name == 'pbmc_thaw_wash_viability':\n            lcl = 80\n            ucl = 98\n        # Notice that these values for PBMC are the same as frozen, \n        # since they are considered kinda the same.\n        elif param_name == 'tcell_activation_density_pbmc':\n            lcl = 0.70e6\n            ucl = 1.46e6\n        elif param_name == 'tcell_activation_density_frozen':\n            lcl = 0.70e6\n            ucl = 1.46e6\n        elif param_name == 'tcell_activation_density_fresh':\n            lcl = 1.20e6\n            ucl = 1.60e6\n        elif param_name == 'viability_wash1':\n            lcl = 80\n            ucl = 96\n        elif param_name == 'transduction_postseeding_density':\n            lcl = 0.42e6\n            ucl = 0.64e6\n        elif param_name == 'viability_wash2':\n            lcl = 89\n            ucl = 97\n        elif param_name == 'tcell_min_expansion_viability':\n            lcl = 85\n            ucl = 97\n        # These values were updated after after AGILE-CC-01156\n        elif param_name == 'fold_expansion_per_day':\n            lcl = 0.6\n            ucl = 2.4\n        elif param_name == 'recovery_wash3':\n            lcl = 69\n            ucl = 115\n\n        # OOS\n        elif param_name == 'viability_harvest':\n            lcl = 84\n            ucl = 97\n        elif param_name == 'transduction_harvest':\n            lcl = 38\n            ucl = 84\n        elif param_name == 'ifng':\n            lcl = 760\n            ucl = 13976\n        elif param_name == 'cd3_cells_p':\n            lcl = 91\n            ucl = 100\n        # VCN by ddPCR\n        elif (param_name == 'vcn_bag1') or (param_name == 'vcn_bag2'):\n            lcl = 2.28\n            ucl = 7.89\n        else:\n            raise Exception(f\"Control limits for YESCARTA parameter {param_name} not defined\")\n\n\n    elif (product == 'tecartus') or (product == 'xlp'):\n        # Exceptions\n        if param_name == 'apheresis_viability':\n            lcl = 97.6\n            ucl = 99.9\n        elif param_name == 'viability_postwash2':\n            lcl = 95.4\n            ucl = 99.5\n        elif param_name == 'postclinimacs_viability':\n            lcl = 91.9\n            ucl = 99.2\n        elif param_name == 'viability_postwash3':\n            lcl = 89.7\n            ucl = 99.2\n        elif param_name == 'excess_tcell_viability_postwash':\n            lcl = np.nan\n            ucl = np.nan\n        elif param_name == 'vcd_seeding_postactivation':\n            lcl = 1.3e6\n            ucl = 1.75e6\n        elif param_name == 'viability_postwash4':\n            lcl = 79.9\n            ucl = 97\n        elif param_name == 'vcd_seeding_posttransduction':\n            lcl = 0.73e6\n            ucl = 1.12e6\n        elif param_name == 'tcell_min_expansion_viability':\n            lcl = 73.2\n            ucl = 95.3\n        elif param_name == 'fold_expansion_per_day':\n            lcl = 0.9\n            ucl = 2.5\n        elif param_name == 'harvest_wash_recovery':\n            lcl = 76\n            ucl = 120\n        elif param_name == 'vcd_average_final_formulation':\n            lcl = 1.1e6\n            ucl = 4.7e6\n\n        # OOS\n        elif param_name == 'viability_harvest':\n            lcl = 75\n            ucl = 95    \n        elif param_name == 'transduction_harvest':\n            lcl = 53\n            ucl = 86\n        elif param_name == 'ifng':\n            lcl = 2133\n            ucl = 16504  \n        elif (param_name == 'vcn_bag1') or (param_name == 'vcn_bag2'):\n            lcl = 3.43\n            ucl = 8.79\n        elif param_name == 'cd3_cells_p':\n            lcl = 97\n            ucl = 100\n\n        else:\n            raise Exception(f\"Control limits for TECARTUS parameter {param_name} not defined\")\n\n    else:\n        raise Exception(f\"{product} is not a defined product\")\n\n    return lcl, ucl\n</code></pre>"},{"location":"api_reference/deviations/#kipy.get_oos_cols--see-also","title":"See also","text":"<ul> <li><code>kipy.classify_deviation</code></li> </ul> Source code in <code>kipy\\deviations.py</code> <pre><code>def get_oos_cols(product):\n    \"\"\"`get_oos_cols`\n\n    Get the name of columns that will be evaluated for OOS.\n\n    Parameters\n    ----------\n    product : str\n        The product of interest. Possible values are:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    Returns\n    -------\n    cols_oos : list\n        List of the column names that will be evaluated for OOS.\n\n    See also\n    --------\n    * [`kipy.classify_deviation`][]\n    \"\"\"\n\n    if (product == 'yescarta') or (product == 'clp'):\n        cols_oos = ['ifng',\n                    'cd3_cells_p',\n                    'transduction_harvest',\n                    'viability_harvest']\n\n    elif (product == 'tecartus') or (product == 'xlp'): \n        cols_oos = ['cd3_cells_p', \n                    'ifng', \n                    'transduction_harvest',\n                    'vcn_bag1', \n                    'vcn_bag2',\n                    'viability_harvest']\n\n    else:\n        raise Exception(f\"{product} is not a valid product.\")\n\n    return cols_oos\n</code></pre>"},{"location":"api_reference/deviations/#kipy.get_excursion_cols--see-also","title":"See also","text":"<ul> <li><code>kipy.classify_deviation</code></li> </ul> Source code in <code>kipy\\deviations.py</code> <pre><code>def get_excursion_cols(product):\n    \"\"\"`get_excursion_cols`\n\n    Get the name of columns that will be evaluated for excursions.\n\n    Parameters\n    ----------\n    product : str\n        The product of interest. Possible values are:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    Returns\n    -------\n    cols_excursion : list\n        List of the column names that will be evaluated for excursions.\n\n    See also\n    --------\n    * [`kipy.classify_deviation`][]\n    \"\"\"\n\n    if (product == 'yescarta') or (product == 'clp'):\n        cols_excursion = ['neatcell_viability',\n                          'neatcell_recovery',\n                          'pbmc_thaw_wash_viability',\n                          'tcell_activation_density',\n                          'viability_wash1',\n                          'transduction_postseeding_density',\n                          'viability_wash2',\n                          'tcell_min_expansion_viability',\n                          'fold_expansion_per_day',\n                          'recovery_wash3'\n                          ]\n\n    elif (product == 'tecartus') or (product == 'xlp'):\n        cols_excursion = ['apheresis_viability', \n                          'excess_tcell_viability_postwash', \n                          'excess_tcell_viability_postthaw', \n                          'viability_postwash2', \n                          'viability_postwash3', \n                          'viability_postwash4', \n                          'vcd_seeding_postactivation',\n                          'vcd_seeding_posttransduction',\n                          'tcell_min_expansion_viability', \n                          'fold_expansion_per_day',\n                          'harvest_wash_recovery', \n                          'postclinimacs_viability', \n                          'vcd_average_final_formulation']\n\n    else:\n        raise Exception(f\"{product} is not a valid product.\")\n\n    return cols_excursion\n</code></pre>"},{"location":"api_reference/deviations/#kipy.get_deviation_cols_extra--see-also","title":"See also","text":"<ul> <li><code>kipy.classify_deviation_extra</code></li> </ul> Source code in <code>kipy\\deviations.py</code> <pre><code>def get_deviation_cols_extra(product):\n    \"\"\" `get_deviation_cols_extras`\n\n    Get the name of additional columns that will be evaluated for deviations,\n    but aren't strictly either OOS or excursions.\n\n    Parameters\n    ----------\n    product : str\n        The product of interest. Possible values are:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    Returns\n    -------\n    cols_extra : list\n        List of the column names that will be evaluated for deviations\n        that are neither OOS or excursions.\n\n    See also\n    --------\n    * [`kipy.classify_deviation_extra`][]\n    \"\"\"\n    if (product == 'yescarta') or (product == 'clp') or (product == 'tecartus') or (product == 'xlp'): \n        cols_extra = ['vi_bag1', 'vi_bag2']\n    else:\n        raise Exception(f\"{product} is not a valid product.\")\n\n    return cols_extra\n</code></pre>"},{"location":"api_reference/deviations/#kipy.classify_deviation--see-also","title":"See also","text":"<ul> <li><code>kipy.get_oos_cols</code></li> <li><code>kipy.get_excursion_cols</code></li> </ul> Source code in <code>kipy\\deviations.py</code> <pre><code>def classify_deviation(product, df, param, verbose=True):\n    \"\"\"`classify_deviation` \n\n    Calculate if a parameter has a deviation (i.e., either excursion or\n    OOS) or not.\n\n    !!! tip \n        Remember deviations are defined based on action limits \n        (and *not* on control limits).\n\n    Parameters\n    ----------\n    product : str\n        The product of interest. Possible values are:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    df : pandas DataFrame\n        Original DataFrame.\n\n    param : string\n        Parameter name. Possible values are the columns defined in \n        [`kipy.get_oos_cols`][] and [`kipy.get_excursion_cols`][]\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df_deviation : pandas DataFrame\n        A copy of the input DataFrame with an extra boolean column named\n        `param + _deviation` that defines if a row was a deviation (True) \n        or not (False).\n\n    See also\n    --------\n    * [`kipy.get_oos_cols`][]\n    * [`kipy.get_excursion_cols`][]\n    \"\"\"\n\n    if verbose:\n        print(f\"Classifying deviation for column {param}...\", end='', flush=True)\n\n    # Get the excursion and the OOS columns.\n    cols_excursion = get_excursion_cols(product)\n    cols_oos = get_oos_cols(product)\n\n    if param in cols_excursion:\n        suffix = '_excursion'\n    elif param in cols_oos:\n        suffix = '_oos'\n    else:\n        suffix = ''\n\n    col_name_deviation = param + suffix\n\n    # Create a new copy of the DataFrame.\n    df_deviation = df.copy(deep=True)\n\n\n    def _classify_deviation(row, param_name):\n\n        param_value = row[param_name]\n\n        # Since the limits of tcell_activation_density depend on the\n        # apheresis_type, we need to incorporate the corresponding suffix.\n        if param_name == 'tcell_activation_density':\n            param_name = param_name + '_' + row['apheresis_type']\n\n        # Get the limits.\n        lower_limit, upper_limit = get_action_limits(product, param_name)\n\n        # Evaluate if the parameter is below/above the lower/upper limit.\n        # if (param_value &lt; lower_limit) or (param_value &gt; upper_limit):\n        if ((param_value &lt; lower_limit) or (param_value &gt; upper_limit)) and (param_value &gt;= 0):\n            deviation = True\n        else:\n            deviation = False\n\n        return deviation\n\n    df_deviation[col_name_deviation] = df.apply(_classify_deviation, args=(param,), axis=1)\n\n    if verbose:\n        print(\"\\t DONE!\")\n\n    return df_deviation\n</code></pre>"},{"location":"api_reference/deviations/#kipy.classify_deviation_extra--see-also","title":"See also","text":"<ul> <li><code>kipy.get_deviation_cols_extra</code></li> </ul> Source code in <code>kipy\\deviations.py</code> <pre><code>def classify_deviation_extra(product, df, param, verbose=True):\n    \"\"\" `classify_deviation_extra`\n\n    Calculate if an extra parameter has a deviation or not.\n\n    !!! info\n        These so-called extra parameters are those that are neither an \n        excursion (related to IPCs) or an OOS (related to CQAs) since they\n        aren't defined as neither. However, they are still asked as part\n        of the process monitoring.\n\n    Parameters\n    ----------\n    product : str\n        The product of interest. Possible values are:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    df : pandas DataFrame\n        Original DataFrame.\n\n    param : string\n        Parameter name. Possible values are the columns defined in \n        [`kipy.get_deviation_cols_extra`][] plus `vi_bags` and `vcn_bags`.\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df_deviation : pandas DataFrame\n        A copy of the input DataFrame with an extra column named\n        `col_name_deviation` that defines if a row was a deviation (True) \n        or not (False).\n\n    See also\n    --------\n    * [`kipy.get_deviation_cols_extra`][]\n    \"\"\"  \n\n    if verbose:\n        print(f\"Classifiying deviation for (extra) column {param}...\", end='', flush=True)\n\n    # Create a new copy of the DataFrame.\n    df_deviation = df.copy(deep=True)\n\n    if param == 'vcn_bags':\n\n        # So far, we evaluate VCN only for TECARTUS.\n        if (product == 'yescarta') or (product == 'clp'):\n            print(\"\\n\\tvcn_bags is not being evaluated for YESCARTA\")\n\n        elif (product == 'tecartus') or (product == 'xlp'):\n            if ('vcn_bag1_oos' in df_deviation.columns) &amp; ('vcn_bag2_oos' in df_deviation.columns):\n                df_deviation[param + '_oos'] = df_deviation['vcn_bag1_oos'] &amp; df_deviation['vcn_bag2_oos']\n            else:\n                raise Exception(f\"Cannot classify {param}. Either column vcn_bag1 or vcn_bag2 are missing.\")\n        else:\n            raise Exception(f\"{product} is not a valid product.\")\n\n    elif (param == 'vi_bag1') or (param == 'vi_bag2'):\n\n        def _classify_oos_vi(param_):\n            if param_ == 'fail':\n                oos = True\n            elif param_ == 'pass':\n                oos = False\n            elif np.isnan(param_):\n                oos = np.nan\n            else:\n                raise Exception(f\"Unexpected parameter value in {param}\")\n\n            return oos\n        df_deviation[param + '_deviation'] = df.apply(lambda x: _classify_oos_vi(x[param]), axis=1)\n\n    elif param == 'vi_bags':\n        if ('vi_bag1_deviation' in df_deviation.columns) &amp; ('vi_bag2_deviation' in df_deviation.columns):\n            df_deviation[param + '_deviation'] = df_deviation['vi_bag1_deviation'] &amp; df_deviation['vi_bag2_deviation']\n        else:\n            raise Exception(f\"Cannot classify {param}. Either column vi_bag1_deviation or vi_bag2_deviation are missing.\")\n\n    else:\n        raise Exception(\"Invalid parameter name.\")\n\n    if verbose:\n        print(\"\\t DONE!\")\n\n    return df_deviation\n</code></pre>"},{"location":"api_reference/deviations/#kipy.get_batches_oos_list--see-also","title":"See also","text":"<ul> <li><code>kipy.classify_deviation</code></li> <li><code>kipy.classify_deviation_extra</code></li> <li><code>kipy.count_deviations</code></li> </ul> Source code in <code>kipy\\deviations.py</code> <pre><code>def get_batches_oos_list(df, verbose=True):\n    \"\"\"`get_batches_oos_list`\n\n    Obtain a DataFrame with a of batches and a list of their OOS.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame with `_oos` columns as obtained from functions\n        [`kipy.classify_deviation`][], [`kipy.classify_deviation_extra`][],\n        and [`kipy.count_deviations`][].\n\n    verbose : bool\n        Define if verbose output will be printed (True) or not (False).\n\n    Returns\n    -------\n    df_oos_cases : pandas DataFrame\n        DataFrame with the batches with OOS. Each row is a batch.\n        It has the following columns:\n\n        | Parameter name              | Output DataFrame name |\n        |-----------------------------|-----------------------|\n        | `batch_id`                  | Batch ID              |\n        | `production_date_formatted` | Production date       |\n        | `oos_description`           | OOS list              |\n        | `status`                    | Status                |\n\n        !!! warning\n            Do *not* change the names of the columns of the output DataFrame.\n            If they are changed, they will also need to be updated in\n            the Power BI dashboard.\n\n\n    See also\n    ----------\n    * [`kipy.classify_deviation`][]\n    * [`kipy.classify_deviation_extra`][]\n    * [`kipy.count_deviations`][]\n    \"\"\"\n    if verbose:\n        print(\"Getting list of batches with OOS...\", end=\"\", flush=True)\n\n    # Get batches that have an OOS.\n    df_oos_cases = df.query('oos == True').copy()\n\n    # Get OOS columns.\n    cols_all = list(df_oos_cases.columns)\n    cols_oos = misc.get_elements_with_substring(cols_all, ['_oos'])\n\n    def _get_oos_list(row):\n\n        # List of OOS will be appended here.\n        oos_description = ''\n\n        # Loop through OOS columns and format their name into \n        # text to be appendend.\n        for col_oos in cols_oos:\n            if row[col_oos] == True:\n                oos_description = oos_description + col_oos.replace('_oos', '').replace('_', ' ').capitalize() + ', '\n\n        # Remove last comma and space.\n        if oos_description != '':\n            oos_description = oos_description[:-2]\n\n        return oos_description\n\n    # If there are batches with OOS, fill the DataFrame.\n    if len(df_oos_cases) &gt;= 1:\n\n        # Get the list of OOS.\n        df_oos_cases['oos_description'] = df_oos_cases.apply(lambda x: _get_oos_list(x), axis=1)\n\n        # Get the status.\n        df_oos_cases['status'] = df_oos_cases['status'].apply(lambda x: x.capitalize())\n\n        # Return only a select number of columns (to match the report structure).\n        df_oos_cases = df_oos_cases[['batch_id', 'production_date_formatted', 'oos_description', 'status']]\n\n        # Rename columns (to match report structure)\n        df_oos_cases.rename(columns={'batch_id':'Batch ID', \n                                     'production_date_formatted':'Production date',\n                                     'oos_description':'OOS list',\n                                     'status':'Status'}, inplace=True)\n\n    # If not, return an empty DataFrame.\n    else:\n        df_oos_cases = pd.DataFrame(columns=['Batch ID', 'Production date', 'OOS list', 'Status'])\n\n    if verbose:\n        print(\"\\tDONE!\")\n\n    return df_oos_cases\n</code></pre>"},{"location":"api_reference/deviations/#kipy.get_deviation_summary--see-also","title":"See also","text":"<ul> <li><code>kipy.classify_deviation</code></li> <li><code>kipy.classify_deviation_extra</code></li> <li><code>kipy.count_deviations</code></li> <li><code>kipy.get_date_end</code></li> </ul> Source code in <code>kipy\\deviations.py</code> <pre><code>def get_deviation_summary(df, deviation_type, partitioned=False, date_end=None, offset=None, verbose=True):\n    \"\"\"`get_deviation_summary`\n\n    Get a summary with the number and percentage of batches with deviations\n    (OOS or exception) of a given DataFrame.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame with `_oos` columns as obtained from functions\n        [`kipy.classify_deviation`][], [`kipy.classify_deviation_extra`][],\n        and [`kipy.count_deviations`][].\n\n    deviation_type : str\n        Define which type of deviations will be counted. Possible values are: \n\n        * `'oos'`\n\n        * `'excursion'`\n\n    partitioned : bool\n        If `True`, the data will be partitioned into past and present\n        (thus `date_end` is a mandatory input). If `False`, the summary will \n        be provided for all data.\n\n    date_end : Given by `kipy.get_date_end`\n        End date as given by [`kipy.get_date_end`][]\n\n    offset : int\n        Offset for `date_start`. \n\n        In case `date_end` corresponds to a monthly resolution, `offset` is \n        in months. \n\n        In case `date_end` corresponds to a quarterly resolution, `offset` is \n        in quarters.\n\n        !!! tip\n            For the current Process Monitoring format, `offset` should be set to:\n\n            * For `monthly`, 6 (i.e., six months)\n            * For `quarterly`, 2 (i.e., six months)\n\n    verbose : bool\n        Define if verbose output will be printed (True) or not (False).\n\n    Returns\n    -------\n    df_deviation_summary : pandas DataFrame\n        For each OOS/excursion column, the output DataFrame will have\n        a `_n` and a `_p` row for number and percentage, respectively.\n\n        In case `partitioned == True`, additionally there will be a `past_`\n        and a `present_` case.\n\n    See also\n    ----------\n    * [`kipy.classify_deviation`][]\n    * [`kipy.classify_deviation_extra`][]\n    * [`kipy.count_deviations`][]\n    * [`kipy.get_date_end`][]\n    \"\"\"\n\n    if verbose:\n        if partitioned:\n            print(f\"Getting batches' {deviation_type} summary (partitioned)...\")\n        else:\n            print(f\"Getting batches' {deviation_type} summary...\")\n\n    # Get the names of the relevant columns.\n    cols_all = list(df.columns)\n    cols_deviations = misc.get_elements_with_substring(cols_all, ['_' + deviation_type])\n\n\n    # If needed, partition the data between past and present.\n    if partitioned:\n\n        # Notice that we need offset date_start here!\n        date_start, date_partition = misc.get_date_start_partition(date_end, offset=offset)\n\n        if verbose:\n            print(f\"\\tBeginning date: {date_start}\")\n            print(f\"\\tPartition date: {date_partition}\")\n            print(f\"\\tEnd date: {date_end}\")\n\n        # Quarter\n        if '-q' in str(date_end):\n            df_past = df.query('(production_date_q &gt;= @date_start) and (production_date_q &lt;= @date_partition)').copy()\n            df_present = df.query('production_date_q &gt; @date_partition').copy()\n\n        # Month\n        else:\n            df_past = df.query('(production_date_short_formatted &gt;= @date_start) &amp; (production_date_short_formatted &lt;= @date_partition)').copy()\n            df_present = df.query('production_date_short_formatted &gt; @date_partition').copy()\n\n        dfs = [df_past, df_present]\n        prefixes = ['past_', 'present_']\n\n    # If not, just put the DataFrame in the required shape.\n    else:\n        dfs = [df]\n        prefixes = ['']\n\n\n    deviation_summary = {}\n\n    # Loop through all DataFrames:\n    # If partitioned, through past and present\n    # If not partitioned, through the single DataFrame.\n    for df_, prefix in zip(dfs, prefixes):\n\n        n_batches = len(df_)\n\n        # Loop through all the column deviations.\n        for col_deviations in cols_deviations:\n\n            # Skip individual bags VCN.\n            if (col_deviations == 'vcn_bag1_oos') or (col_deviations == 'vcn_bag2_oos'):\n                continue\n\n            deviation_summary[prefix + col_deviations + '_n'] = sum(df_[col_deviations])\n\n            if n_batches &gt; 0:\n                deviation_summary[prefix + col_deviations + '_p'] = round((deviation_summary[prefix + col_deviations + '_n'] / n_batches) * 100, 2)\n            else:\n                deviation_summary[prefix + col_deviations + '_p'] = np.nan\n\n    # Transform dictionary to DataFrame\n    df_deviation_summary = pd.DataFrame(deviation_summary.items(), columns=[deviation_type + '_parameter', 'value'])\n\n    if verbose:\n        print(\"\\t\\t\\tDONE!\")\n\n    return df_deviation_summary\n</code></pre>"},{"location":"api_reference/feat_engineering/","title":"Feature Engineering","text":"<p>These functions add useful columns to the original (pre-processed) data.</p> <p><code>add_engineered_columns</code></p> <p>Add a set of engineered columns to a given <code>CLP</code>/<code>XLP</code> DataFrame.</p> <p>Info</p> <p>All products get the same new columns. The only difference is how these are calculated. More specifically, the <code>production_date</code> definition is different between YESCARTA and TECARTUS. For more information, see <code>kipy.add_production_date</code></p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>Original DataFrame.</p> required <code>product</code> <code>str</code> <p>The product of interest. It accepts the following values:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas DataFrame</code> <p>Same as the input DataFrame but with added columns</p> <ul> <li><code>production_date</code></li> <li><code>production_date_numeric</code></li> <li><code>production_date_formatted</code></li> <li><code>production_date_q_formatted</code></li> <li><code>production_date_q</code></li> <li><code>production_date_quarter</code></li> <li><code>production_date_month</code></li> <li><code>production_date_month_name</code></li> <li><code>production_date_year</code></li> <li><code>production_date_short</code></li> <li><code>production_date_short_formatted</code></li> <li><code>harvested</code></li> <li><code>harvest_day</code></li> <li><code>attempt_number</code></li> <li><code>attempt_type</code></li> <li><code>verified</code></li> <li><code>success</code></li> <li><code>tcell_min_expansion_viability</code></li> </ul> Source code in <code>kipy\\feat_engineering.py</code> <pre><code>def add_engineered_columns(df, product, verbose=True):\n    \"\"\"`add_engineered_columns`\n\n    Add a set of engineered columns to a given `CLP`/`XLP` DataFrame.\n\n    !!! info\n        All products get the same new columns. The only difference is\n        *how* these are calculated. More specifically, the `production_date`\n        definition is different between YESCARTA and TECARTUS. For\n        more information, see [`kipy.add_production_date`][]\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        Original DataFrame.\n\n    product : str\n        The product of interest. It accepts the following values:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df : pandas DataFrame\n        Same as the input DataFrame but with added columns\n\n        * `production_date`\n        * `production_date_numeric`\n        * `production_date_formatted`\n        * `production_date_q_formatted`\n        * `production_date_q`\n        * `production_date_quarter`\n        * `production_date_month`\n        * `production_date_month_name`\n        * `production_date_year`\n        * `production_date_short`\n        * `production_date_short_formatted`\n        * `harvested`\n        * `harvest_day`\n        * `attempt_number`\n        * `attempt_type`\n        * `verified`\n        * `success`\n        * `tcell_min_expansion_viability`\n\n    \"\"\"\n\n    # Since verbose is handled internally for each function, there's \n    # no need to add any extra prints here.\n    df_production_date = add_production_date(df, product, verbose=verbose)\n    df_production_dates = add_production_date_columns(df_production_date, verbose=verbose)\n    df_harvested = add_harvested(df_production_dates, verbose=verbose)\n    df_harvest_day = add_harvest_day(df_harvested, verbose=verbose)\n    df_attempts = add_attempt_number_type(df_harvest_day, verbose=verbose)\n    df_verified = add_verified(df_attempts, verbose=verbose)\n    df_tcell = add_tcell_min_expansion_viability(df_verified, verbose=verbose)\n    df_engineered = add_success(df_tcell, verbose=verbose)\n\n    return df_engineered\n</code></pre> <p><code>add_production_date</code></p> <p>Add <code>production_date</code> column.</p> <p>Info</p> <p>In the past, <code>production_date</code> was defined as <code>harvest_date</code>. If the batch had no <code>harvest_date</code> (e.g., if the batch was terminated before harvest), we used <code>start_day0_date</code>. </p> <p>However, this definition is no longer valid. Now, the definition of  <code>production_date</code> is different for YESCARTA and TECARTUS, as shown  in the following table.</p> Product <code>production_date</code> definition YESCARTA Given by the so-called \"JMP scripts logic\": find the estimated processing duration (in days) and add it to <code>start_day0_date</code>. TECARTUS Given by the column <code>harvest_termination_date</code> <p>In the future, the YESCARTA definition might align with that from TECARTUS, but it will depend on the results of the discussion  regarding that column (still ongoing).</p> <p>Danger</p> <p>If the definition of <code>production_date</code> changes in the future, this  function needs to be updated.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>Original DataFrame</p> required <code>product</code> <code>str</code> <p>The product of interest. It accepts the following values:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_production_date</code> <code>pandas DataFrame</code> <p>Original DataFrame with the column <code>production_date</code> added.</p> <p><code>add_production_date_columns</code></p> <p>Add formatted production date columns to a DataFrame.</p> <p>Example</p> <p>If production_date is <code>15-Feb-22</code>, the following columns will be added:</p> Column name Description Output <code>production_date_numeric</code> Production date in numeric format <code>15-02-22</code> <code>production_date_formatted</code> Production date in proper datetime format <code>2022-02-15 00:00:00</code> <code>production_date_q_formatted</code> Production date as quarter in proper period format <code>2022Q1</code> <code>production_date_q</code> Production date as quarter <code>2022-q1</code> <code>production_date_quarter</code> Production date quarter number <code>1</code> <code>production_date_month</code> Production month as a string with leading zero if necessary <code>02</code> <code>production_date_month_name</code> Production month name <code>'February'</code> <code>production_date_year</code> Production year as a string <code>2022</code> <code>production_date_short</code> Production date with only year and month as string <code>2022-02</code> <code>production_date_short_formatted</code> Production date with only year and month in proper datetime format <code>2022-02-01 00:00:00</code> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>Original DataFrame</p> required <code>col_production_date</code> <code>str</code> <p>DataFrame column where the production date information is stored. This column should be in the format <code>DD-MMM-YY</code> (as stored in the <code>CLP</code>/<code>XLP</code> files).</p> <code>'production_date'</code> <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_production_date</code> <code>pandas DataFrame</code> <p>DataFrame with the extra columns. See Examples.</p> <p><code>add_success</code></p> <p>Classify if a batch was successful (<code>True</code>) or not (<code>False</code>) based on the  status information of the given column.</p> <p>Danger</p> <p>If the definition of <code>success</code> changes in the future, this function  needs to be updated.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>Original DataFrame</p> required <code>col_status</code> <code>str</code> <p>DataFrame column where the status date is stored.</p> <code>'status'</code> <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas DataFrame</code> <p>Same as the input DataFrame but with an added column <code>success</code>.</p> Source code in <code>kipy\\feat_engineering.py</code> <pre><code>def add_success(df, col_status='status', verbose=True):\n    \"\"\"`add_success`\n\n    Classify if a batch was successful (`True`) or not (`False`) based on the \n    status information of the given column.\n\n    !!! danger\n\n        If the definition of `success` changes in the future, this function \n        needs to be updated.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        Original DataFrame\n\n    col_status : str\n        DataFrame column where the status date is stored.\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df: pandas DataFrame\n        Same as the input DataFrame but with an added column `success`.\n    \"\"\"\n    if 'success' in df.columns:\n        if verbose:\n            print(\". Column `success` already present in df.\")\n\n    else:\n        if col_status not in df.columns:\n            raise Exception(f\"Column {col_status} is not present in df.\")\n\n        if verbose:\n            print(\"+ Adding column `success`...\", end='', flush=True)\n\n        def _classify_success(status):\n\n            if pd.isnull(status):\n                success = np.nan\n            else:\n                curr_status = status.lower().strip()\n                if (curr_status == 'release') or (curr_status == 'physician\\'s release'):\n                    success = True\n                else:\n                    success = False\n\n            return success\n\n        df['success'] = df.apply(lambda x: _classify_success(x[col_status]), axis=1)\n\n        if verbose:\n            print(\"\\tDONE!\")\n\n    return df\n</code></pre> <p><code>add_harvest_day</code></p> <p>Add the harvest day of a batch, which is calculated based on  <code>volume_adjusted_dayX</code> columns. </p> <p>Info</p> <p>Notice that this only applies to YESCARTA (<code>CLP</code>), since TECARTUS  (<code>XLP</code>) already has the <code>harvest_day</code> column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>Original DataFrame</p> required <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas DataFrame.</code> <p>Same as the input DataFrame but with an added column <code>harvest_day</code>.</p> Source code in <code>kipy\\feat_engineering.py</code> <pre><code>def add_harvest_day(df, verbose=True):\n    \"\"\"`add_harvest_day`\n\n    Add the harvest day of a batch, which is calculated based on \n    `volume_adjusted_dayX` columns. \n\n    !!! info\n        Notice that this only applies to YESCARTA (`CLP`), since TECARTUS \n        (`XLP`) already has the `harvest_day` column.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        Original DataFrame\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df : pandas DataFrame.\n        Same as the input DataFrame but with an added column `harvest_day`.\n    \"\"\"\n\n    # Check if harvest_day already exists (which is the case in XLP)\n    if 'harvest_day' in df.columns:\n        if verbose:\n            print(\". Column `harvest_day` already present in df.\")\n    else:        \n        if verbose:\n            print(\"+ Adding column `harvest_day`...\", end='', flush=True)\n\n        # These are the necessary columns to compute harvest_day.\n        volume_columns = ['volume_adjusted_day7', 'volume_adjusted_day8', 'volume_adjusted_day9', 'volume_adjusted_day10']\n\n        if not set(volume_columns).issubset(df.columns):\n            # If the necessary columns are not present, raise exception.\n            raise Exception(\"Volume columns (volume_adjusted_dayX) are not present in the df.\")\n        else:\n            # If the necessary columns are present, actually compute harvest_day.\n            df['harvest_day'] = (df[volume_columns].count(axis=1)) + 6\n\n            # If a batch had a harvest_day of 6, it means that all the\n            # volume_adjusted_dayX columns were empty. This means that\n            # the batch was actually not harvested. Thus, we need\n            # to fill those values with NaNs.\n            df.loc[df['harvest_day'] == 6, 'harvest_day'] = np.nan\n\n        if verbose:\n            print(\"\\tDONE!\")\n\n    return df\n</code></pre> <p><code>add_attempt_number_type</code></p> <p>Add the attempt number and attempt type to a DataFrame.</p> <p>Info</p> <p>There are two reasons why a batch could have gone through more than one attempt. These (and their corresponding original definition) are: </p> <ul> <li> <p>Re-manufacture (<code>reman</code>): batches that share the same <code>subject_id</code>,      but have slightly different <code>batch_id</code>, changing only the first      character (e.g., <code>a483965890</code> and <code>d483965890</code>).</p> </li> <li> <p>Re-apheresis (<code>reaph</code>): batches that share the same <code>subject_id</code>,      but have completely different <code>batch_id</code> (e.g., <code>a483965890</code> and <code>a529326491</code>)</p> </li> </ul> <p>However, identifying them using this simple definition is not very robust  in cases when there are more than 2 attempts. Therefore, we will identify  them in a different way (as suggested by Diana), as follows:</p> <ol> <li>Sort batches by <code>production_date_formatted</code>.      This step is super important to ensure that earliest batches will      always be higher on the DataFrame!</li> <li>Identify batches that have duplicated <code>subject_id</code>, since those are      the ones that have more than one attempt.</li> <li>Group the batches by <code>subject_id</code> and add an ordinal number that     represent the order in which they appeared. This will be the      batch's <code>attempt_number</code>.</li> <li>If a batch had an <code>attempt_number</code> of <code>1</code>, then it is straightforward     and its <code>attempt_type</code> is <code>first</code>.</li> <li>For the rest of batches with multiple attempts, we check if the      batch had an apheresis using the column      <code>apheresis_spiked_welded_date_time</code>. If it has a non-NaN      apheresis date, we classify it as <code>reaph</code>, otherwise, we classify      it as <code>reman</code>.</li> <li>All other batches with a single attempt are given an      <code>attempt_number</code> of 1 and thus an <code>attempt_type</code> of <code>first</code>.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <p>Original DataFrame.</p> required <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (True) or not (False). Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas DataFrame</code> <p>Same as the input DataFrame but with added columns <code>attempt_number</code> and <code>attempt_type</code>.</p> Source code in <code>kipy\\feat_engineering.py</code> <pre><code>def add_attempt_number_type(df, verbose=True):\n    \"\"\"`add_attempt_number_type`\n\n    Add the attempt number and attempt type to a DataFrame.\n\n    !!! info\n        There are two reasons why a batch could have gone through more than one\n        attempt. These (and their corresponding original definition) are: \n\n        * Re-manufacture (`reman`): batches that share the same `subject_id`, \n            but have slightly different `batch_id`, changing only the first \n            character (e.g., `a483965890` and `d483965890`).\n\n        * Re-apheresis (`reaph`): batches that share the same `subject_id`, \n            but have completely different `batch_id` (e.g., `a483965890` and `a529326491`)\n\n        However, identifying them using this simple definition is not very robust \n        in cases when there are more than 2 attempts. Therefore, we will identify \n        them in a different way (as suggested by Diana), as follows:\n\n        1. Sort batches by `production_date_formatted`. \n            This step is super important to ensure that earliest batches will \n            always be higher on the DataFrame!\n        2. Identify batches that have duplicated `subject_id`, since those are \n            the ones that have more than one attempt.\n        3. Group the batches by `subject_id` and add an ordinal number that\n            represent the order in which they appeared. This will be the \n            batch's `attempt_number`.\n        4. If a batch had an `attempt_number` of `1`, then it is straightforward\n            and its `attempt_type` is `first`.\n        5. For the rest of batches with multiple attempts, we check if the \n            batch had an apheresis using the column \n            `apheresis_spiked_welded_date_time`. If it has a non-NaN \n            apheresis date, we classify it as `reaph`, otherwise, we classify \n            it as `reman`.\n        6. All other batches with a single attempt are given an \n            `attempt_number` of 1 and thus an `attempt_type` of `first`.\n\n    Parameters\n    ----------\n    df: pandas DataFrame\n        Original DataFrame.\n\n    verbose : bool\n        Define if verbose output will be printed (True) or not (False).\n        Defaults to True.\n\n    Returns\n    -------\n    df: pandas DataFrame\n        Same as the input DataFrame but with added columns `attempt_number`\n        and `attempt_type`.\n\n    \"\"\"\n    if ('attempt_number'  not in df.columns) and ('attempt_type'  not in df.columns):\n\n        # Check that necessary columns are present in the DataFrame.\n        cols_needed = ['batch_id', 'subject_id', 'production_date_formatted']\n        for col_ in cols_needed:\n            if col_ not in df.columns:\n                raise Exception(f\"Column {col_} not present in df. Needed for calculating `attempt_number` and `attempt_number`.\")\n\n        # These columns are different for CLP and XLP. Thus, they \n        # need to be processed differently.\n        cols_aph = ['apheresis_spiked_welded_date_time', # CLP\n                    'apheresis_spike_date'] # XLP\n        col_aph = None\n        for col_aph_ in cols_aph:\n            if col_aph_ in df.columns:\n                col_aph = col_aph_\n                break\n\n        if verbose:\n            print(\"+ Adding columns `attempt_number` and `attempt_number`...\", end='', flush=True)\n\n        # 1. Sort batches by `production_date_formatted`. \n        # This step is super important to ensure that earliest batches will \n        # always be higher on the DataFrame!\n        df_sorted_date = df.sort_values(by='production_date_formatted')\n\n        # 2. Identify batches that have duplicated `subject_id`, since those are \n        # the ones that have more than one attempt.\n        df_batches_many_attempts = df_sorted_date[df_sorted_date.duplicated('subject_id', keep=False) == True]\n        df_batches_many_attempts = df_batches_many_attempts.sort_values(by='production_date_formatted')\n\n        # 3. Group the batches by `subject_id` and add an ordinal number that\n        # represent the order in which they appeared. This will be the \n        # batch's `attempt_number`.\n        df_batches_many_attempts['attempt_number'] = df_batches_many_attempts.groupby('subject_id').cumcount()+1\n\n\n        def classify_attempt(row):\n\n            # 4. If a batch had an `attempt_number` of `1`, then it is straightforward\n            # and its `attempt_type` is `first`.    \n            if row['attempt_number'] == 1:\n                attempt = 'first'\n\n            # 5. For the rest of batches with multiple attempts, we check if the \n            # batch had an apheresis using the column \n            # `col_aph`. If it has a non-NaN apheresis date, \n            # we classify it as `reaph`, otherwise, we classify it as `reman`.\n            else:\n                if row[col_aph] == row[col_aph]:\n                    attempt = 'reaph'\n                else:\n                    attempt = 'reman'\n\n            return attempt\n\n        df_batches_many_attempts['attempt_type'] = df_batches_many_attempts.apply(classify_attempt, axis=1)\n\n\n        # 6. All other batches with a single attempt are given an `attempt_number`\n        # of 1 and thus an `attempt_type` of `first`.\n        df = pd.merge(df, df_batches_many_attempts[['batch_id', 'attempt_number', 'attempt_type']], how=\"left\", on=['batch_id'])\n        df['attempt_number'] = df['attempt_number'].fillna(1)\n        df['attempt_type'] = df['attempt_type'].fillna('first')\n\n        if verbose:\n            print(\"\\tDONE!\")\n\n    else:\n        if verbose:\n            print(\". Columns `attempt_number` and `attempt_type` already existed in the DataFrame.\")\n\n    return df\n</code></pre> <p><code>add_verified</code></p> <p>Add a column that identifies if a batch record was verified (<code>True</code>) or not (<code>False</code>). </p> <p>Info</p> <p>A batch is considered verified if both <code>col_recorded</code> and <code>col_verified</code> are not-<code>NaN</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>Original DataFrame.</p> required <code>col_recorded</code> <code>str</code> <p>Name of the column that has the initials of whoever recorded the original data entry.</p> <code>'recorded_by'</code> <code>col_verified</code> <code>str</code> <p>Name of the column that has the initials of whoever veerified the data entry.</p> <code>'verified_by'</code> <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (True) or not (False). Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas DataFrame.</code> <p>Same as the input DataFrame but with an added column 'verified'.</p> Source code in <code>kipy\\feat_engineering.py</code> <pre><code>def add_verified(df, col_recorded='recorded_by', col_verified='verified_by', verbose=True):\n    \"\"\"`add_verified`\n\n    Add a column that identifies if a batch record was verified (`True`)\n    or not (`False`). \n\n    !!! info\n        A batch is considered verified if **both**\n        `col_recorded` and `col_verified` are not-`NaN`.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        Original DataFrame.\n    col_recorded : str\n        Name of the column that has the initials of whoever recorded the\n        original data entry.\n    col_verified : str\n        Name of the column that has the initials of whoever veerified the\n        data entry.\n    verbose : bool\n        Define if verbose output will be printed (True) or not (False).\n        Defaults to True.\n\n    Returns\n    -------\n    df: pandas DataFrame.\n        Same as the input DataFrame but with an added column 'verified'.\n    \"\"\"\n\n    # Check if verified column already exists.\n    if 'verified' in df.columns:\n        if verbose:\n            print(\". Column `verified` already present in df.\")\n    else:        \n        if verbose:\n            print(\"+ Adding column `verified`...\", end='', flush=True)\n\n        df['verified'] = ~(pd.isna(df[col_recorded]) | pd.isna(df[col_verified]))\n\n        if verbose:\n            print(\"\\tDONE!\")\n\n    return df\n</code></pre> <p><code>add_tcell_min_expansion_viability</code></p> <p>Add the T-cell minimal expansion viability column, which is needed for process monitoring (for both YESCARTA and TECARTUS).</p> <p>Info</p> <p><code>tcell_min_expansion_viability</code> is calculated as the minimum of a set of columns. Depending on the product, these are:</p> <code>CLP</code> column <code>XLP</code> column Computer name (same for both) <code>Day 3 Expansion Viability</code> <code>Day 3 T-Cell Expansion Average Cell Viability (%)</code> <code>expansion_viability_day3</code> <code>Day 5 Viability</code> <code>Day 5 Transduced Average Cell Viability (%)</code> <code>viability_day5</code> <code>Day 6 Viability</code> <code>Day 6  Average Cell Viability (%)</code> <code>viability_day6</code> <code>Day 7 Viability</code> <code>Day 7 Post-Expansion Average Cell Viability (%)</code> <code>viability_day7</code> <code>Day 8 Viability</code> <code>Day 8 Post-Expansion Average Cell Viability (%)</code> <code>viability_day8</code> <code>Day 9 Viability</code> <code>Day 9 Post-Expansion Average Cell Viability (%)</code> <code>viability_day9</code> <code>Day 10 Viability</code> <code>Day 10 Post-Expansion Average Cell Viability (%)</code> <code>viability_day10</code> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>Original DataFrame</p> required <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas DataFrame.</code> <p>Same as the input DataFrame but with an added column <code>tcell_min_expansion_viability</code>.</p> Source code in <code>kipy\\feat_engineering.py</code> <pre><code>def add_tcell_min_expansion_viability(df, verbose=True):\n    \"\"\"`add_tcell_min_expansion_viability`\n\n    Add the T-cell minimal expansion viability column, which is needed for\n    process monitoring (for both YESCARTA and TECARTUS).\n\n    !!! info\n        `tcell_min_expansion_viability` is calculated as the *minimum* of\n        a set of columns. Depending on the product, these are:\n\n        | `CLP` column                | `XLP` column                                        | Computer name (same for both) |\n        | ----------------------------|-----------------------------------------------------|-------------------------------|\n        | `Day 3 Expansion Viability` | `Day 3 T-Cell Expansion Average Cell Viability (%)` | `expansion_viability_day3`    |\n        | `Day 5 Viability`           | `Day 5 Transduced Average Cell Viability (%)`       | `viability_day5`              |\n        | `Day 6 Viability`           | `Day 6  Average Cell Viability (%)`                 | `viability_day6`              |\n        | `Day 7 Viability`           | `Day 7 Post-Expansion Average Cell Viability (%)`   | `viability_day7`              |\n        | `Day 8 Viability`           | `Day 8 Post-Expansion Average Cell Viability (%)`   | `viability_day8`              |\n        | `Day 9 Viability`           | `Day 9 Post-Expansion Average Cell Viability (%)`   | `viability_day9`              |\n        | `Day 10 Viability`          | `Day 10 Post-Expansion Average Cell Viability (%)`  | `viability_day10`             |\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        Original DataFrame\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df : pandas DataFrame.\n        Same as the input DataFrame but with an added column `tcell_min_expansion_viability`.\n    \"\"\"\n\n    # Check if verified column already exists.\n    if 'tcell_min_expansion_viability' in df.columns:\n        if verbose:\n            print(\". Column `tcell_min_expansion_viability` already present in df.\")\n\n    else:\n        # Check that required columns are present.\n        required_columns = ['expansion_viability_day3', \n                            'viability_day5', \n                            'viability_day6', \n                            'viability_day7', \n                            'viability_day8', \n                            'viability_day9', \n                            'viability_day10']\n\n        for required_column in required_columns:\n            if not required_column in df.columns:\n                raise Exception(f\"Column {required_column} is not present in df. Needed for calculating tcell_min_expansion_viability.\")\n\n        # Perform the min operation and add the column.\n        if verbose:\n            print(\"+ Adding column `tcell_min_expansion_viability`...\", end='', flush=True)\n\n        df['tcell_min_expansion_viability'] = df.loc[:, ['expansion_viability_day3',\n                                                         'viability_day5', \n                                                         'viability_day6', \n                                                         'viability_day7', \n                                                         'viability_day8', \n                                                         'viability_day9', \n                                                         'viability_day10']].min(axis=1)\n\n        if verbose:\n            print(\"\\tDONE!\")\n\n    return df\n</code></pre>"},{"location":"api_reference/feat_engineering/#kipy.add_production_date--see-also","title":"See also","text":"<ul> <li><code>kipy.add_production_date_columns</code></li> </ul> Source code in <code>kipy\\feat_engineering.py</code> <pre><code>def add_production_date(df, product, verbose=True):\n    \"\"\"`add_production_date`\n\n    Add `production_date` column.\n\n    !!! info\n        *In the past*, `production_date` was defined as `harvest_date`.\n        If the batch had no `harvest_date` (e.g., if the batch was terminated\n        before harvest), we used `start_day0_date`. \n\n        However, this definition is no longer valid. Now, the definition of \n        `production_date` is different for YESCARTA and TECARTUS, as shown \n        in the following table.\n\n        | Product  | `production_date` definition |\n        | ---------| -----------------------------|\n        | YESCARTA | Given by the so-called \"JMP scripts logic\": find the estimated processing duration (in days) and add it to `start_day0_date`. |\n        | TECARTUS | Given by the column `harvest_termination_date` |\n\n        In the future, the YESCARTA definition *might* align with that from\n        TECARTUS, but it will depend on the results of the discussion \n        regarding that column (still ongoing).\n\n    !!! danger\n        If the definition of `production_date` changes in the future, this \n        function needs to be updated.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        Original DataFrame    \n\n    product : str\n        The product of interest. It accepts the following values:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df_production_date : pandas DataFrame\n        Original DataFrame with the column `production_date` added.\n\n    See also\n    --------\n    * [`kipy.add_production_date_columns`][]\n    \"\"\"\n    df_production_date = df.copy(deep=True)\n\n    if 'production_date' in df_production_date.columns:\n        if verbose:\n            print(\". Column `production_date` already present in df.\")\n\n    else:\n        if (product == 'clp') or (product == 'yescarta'):\n            if verbose:\n                print(\"+ Adding column `production_date`...\", end='', flush=True)\n\n            # Estimate the process duration based on finding the last day from\n            # which we have batch data.\n            def get_process_duration(row):\n                if (~np.isnan(row['volume_adjusted_day10'])) or (~np.isnan(row['viability_day10'])):\n                    process_duration = 10\n                elif (~np.isnan(row['volume_adjusted_day9'])) or (~np.isnan(row['viability_day9'])):\n                    process_duration = 9\n                elif (~np.isnan(row['volume_adjusted_day8'])) or (~np.isnan(row['viability_day8'])):\n                    process_duration = 8\n                elif (~np.isnan(row['volume_adjusted_day7'])) or (~np.isnan(row['viability_day7'])):\n                    process_duration = 7\n                elif (~np.isnan(row['culture_volume_day6'])) or (~np.isnan(row['viability_day6'])):\n                    process_duration = 6\n                elif (~np.isnan(row['culture_volume_day5'])) or (~np.isnan(row['viability_day5'])):\n                    process_duration = 5\n                elif (~np.isnan(row['bag_volume_total_expansion'])) or (~np.isnan(row['expansion_viability_day3'])):\n                    process_duration = 3\n                elif (~np.isnan(row['transduction_postseeding_density'])) or (~np.isnan(row['viability_wash1'])):\n                    process_duration = 2\n                else:\n                    process_duration = 0\n\n                return process_duration\n\n            df['process_duration'] = df.apply(get_process_duration, axis=1)\n            df['process_duration'] = pd.to_timedelta(df['process_duration'], unit='D')\n\n            # Calculate production date as `start_day0_date` + `process_duration`.\n            df['production_date'] = pd.to_datetime(df['start_day0_date'], dayfirst=True) + df['process_duration']\n\n            df_production_date['production_date'] = df['production_date'].dt.strftime('%d-%b-%y')\n\n            if verbose:\n                print(\"\\tDONE!\")\n\n\n        elif (product == 'xlp') or (product == 'tecartus'):\n            if not 'harvest_termination_date' in df.columns:\n                raise Exception(\"Column harvest_termination_date not present in DataFrame.\")\n            else:\n                if verbose:\n                    print(\"+ Adding column `production_date`...\", end='', flush=True)\n                df_production_date.loc[:, 'production_date'] = df['harvest_termination_date']\n                if verbose:\n                    print(\"\\tDONE!\")\n\n    return df_production_date\n</code></pre>"},{"location":"api_reference/feat_engineering/#kipy.add_production_date_columns--see-also","title":"See also","text":"<ul> <li><code>kipy.add_production_date</code></li> </ul> Source code in <code>kipy\\feat_engineering.py</code> <pre><code>def add_production_date_columns(df, col_production_date='production_date', verbose=True):\n    \"\"\"`add_production_date_columns`\n\n    Add formatted production date columns to a DataFrame.\n\n    !!! example\n\n        If production_date is `15-Feb-22`, the following columns will be added:\n\n        | Column name                       | Description                                                        | Output                |\n        | --------------------------------- | -------------------------------------------------------------------|-----------------------|\n        | `production_date_numeric`         | Production date in numeric format                                  | `15-02-22`            |\n        | `production_date_formatted`       | Production date in proper datetime format                          | `2022-02-15 00:00:00` |\n        | `production_date_q_formatted`     | Production date as quarter in proper period format                 | `2022Q1`              |\n        | `production_date_q`               | Production date as quarter                                         | `2022-q1`             |\n        | `production_date_quarter`         | Production date quarter number                                     | `1`                   |\n        | `production_date_month`           | Production month as a string with leading zero if necessary        | `02`                  |\n        | `production_date_month_name`      | Production month name                                              | `'February'`          |\n        | `production_date_year`            | Production year as a string                                        | `2022`                |\n        | `production_date_short`           | Production date with only year and month as string                 | `2022-02`             |\n        | `production_date_short_formatted` | Production date with only year and month in proper datetime format | `2022-02-01 00:00:00` |\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        Original DataFrame\n\n    col_production_date : str\n        DataFrame column where the production date information is stored.\n        This column should be in the format `DD-MMM-YY` (as stored\n        in the `CLP`/`XLP` files). \n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df_production_date : pandas DataFrame\n        DataFrame with the extra columns. See Examples.\n\n    See also\n    --------\n    * [`kipy.add_production_date`][]\n    \"\"\"\n\n    # Check that col_production_date is present in the DataFrame.\n    # If not, raise an exception.\n    if col_production_date not in df.columns:\n        raise Exception(f\"Column {col_production_date} is not present in df.\")\n\n\n    df_production_date = df.copy()\n\n    if 'production_date_numeric' not in df.columns:\n        if verbose:\n            print(\"+ Adding column `production_date_numeric`...\", end='', flush=True)\n        df_production_date['production_date_numeric'] = df_production_date[col_production_date].apply(lambda x: pd.to_datetime(x).strftime('%d-%m-%y') if not pd.isnull(x) else np.nan)\n        if verbose:\n            print(\"\\tDONE!\")\n    else:\n        if verbose:\n            print(\". Column `production_date_numeric` already present in the DataFrame\")\n\n    if 'production_date_formatted' not in df.columns:\n        if verbose:\n            print(\"+ Adding column `production_date_formatted`...\", end='', flush=True)\n        df_production_date['production_date_formatted'] = df_production_date['production_date_numeric'].apply(lambda x: pd.to_datetime(x, dayfirst=True) if not pd.isnull(x) else np.nan)\n        if verbose:\n            print(\"\\tDONE!\")\n    else:\n        if verbose:\n            print(\". Column `production_date_formatted` already present in the DataFrame\")            \n\n    if 'production_date_q_formatted' not in df.columns:\n        if verbose:\n            print(\"+ Adding column `production_date_q_formatted`...\", end='', flush=True)\n        df_production_date['production_date_q_formatted'] = df_production_date['production_date_formatted'].apply(lambda x: x.to_period('Q') if not pd.isnull(x) else np.nan)\n        if verbose:\n            print(\"\\tDONE!\")\n    else:\n        if verbose:\n            print(\". Column `production_date_q_formatted` already present in the DataFrame\")    \n\n    if 'production_date_q' not in df.columns:\n        if verbose:\n           print(\"+ Adding column `production_date_q`...\", end='', flush=True)\n        df_production_date['production_date_q'] = df_production_date['production_date_formatted'].apply(lambda x: str(x.year) + '-q' + str(x.quarter) if not pd.isnull(x) else np.nan)\n        if verbose:\n            print(\"\\tDONE!\")\n    else:\n        if verbose:\n            print(\". Column `production_date_q` already present in the DataFrame\")  \n\n    if 'production_date_quarter' not in df.columns:\n        if verbose:\n            print(\"+ Adding column `production_date_quarter`...\", end='', flush=True)\n        df_production_date['production_date_quarter'] = df_production_date['production_date_q_formatted'].apply(lambda x: str(x.quarter) if not pd.isnull(x) else np.nan)\n        if verbose:\n            print(\"\\tDONE!\")\n    else:\n        if verbose:\n            print(\". Column `production_date_quarter` already present in the DataFrame\")  \n\n    if 'production_date_month' not in df.columns:\n        if verbose:\n            print(\"+ Adding column `production_date_month`...\", end='', flush=True)\n        df_production_date['production_date_month'] = df_production_date['production_date_formatted'].apply(lambda x: str(x.month).zfill(2) if not pd.isnull(x) else np.nan) # Ensure month has leading 0.\n        if verbose:\n            print(\"\\tDONE!\")\n    else:\n        if verbose:\n            print(\". Column `production_date_month` already present in the DataFrame\")  \n\n    if 'production_date_month_name' not in df.columns:\n        if verbose:\n            print(\"+ Adding column `production_date_month_name`...\", end='', flush=True)\n        def _month_number_to_name(row):\n            if pd.isna(row['production_date_month']):\n                month_name = '-'\n            else:\n                month_name = calendar.month_name[int(row['production_date_month'])]\n            return month_name\n        df_production_date['production_date_month_name'] = df_production_date.apply(_month_number_to_name, axis=1)    \n        if verbose:\n            print(\"\\tDONE!\")\n    else:\n        if verbose:\n            print(\". Column `production_date_month_name` already present in the DataFrame\") \n\n    if 'production_date_year' not in df.columns:\n        if verbose:\n            print(\"+ Adding column `production_date_year`...\", end='', flush=True)\n        df_production_date['production_date_year'] = df_production_date['production_date_formatted'].apply(lambda x: str(x.year) if not pd.isnull(x) else np.nan)\n        if verbose:\n            print(\"\\tDONE!\")\n    else:\n        if verbose:\n            print(\". Column `production_date_year` already present in the DataFrame\") \n\n    if 'production_date_short' not in df.columns:\n        if verbose:\n            print(\"+ Adding column `production_date_short`...\", end='', flush=True)\n        df_production_date['production_date_short'] = df_production_date['production_date_formatted'].apply(lambda x: str(x.year) + '-' + str(x.month).zfill(2) if not pd.isnull(x) else np.nan)\n        if verbose:\n            print(\"\\tDONE!\")\n    else:\n        if verbose:\n            print(\". Column `production_date_short` already present in the DataFrame\") \n\n    if 'production_date_short_formatted' not in df.columns:\n        if verbose:\n            print(\"+ Adding column `production_date_short_formatted`...\", end='', flush=True)\n        df_production_date['production_date_short_formatted'] = df_production_date['production_date_short'].apply(lambda x: pd.to_datetime(x, format='%Y-%m') if not pd.isnull(x) else np.nan)\n        if verbose:\n            print(\"\\tDONE!\")\n    else:\n        if verbose:\n            print(\". Column `production_date_short_formatted` already present in the DataFrame\") \n\n    return df_production_date\n</code></pre>"},{"location":"api_reference/input_data/","title":"Input Data","text":"<p>These are functions that facilitate reading data operations for further manipulation.</p> <p><code>copy_most_recent_data_file</code></p> <p>Create a time-stamped copy of the most recent data file in the required location.</p> <p>Parameters:</p> Name Type Description Default <code>product</code> <code>str</code> <p>The product of interest. It accepts the following values:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <code>file_type</code> <code>str</code> <p>File type. It accepts the following values:</p> <ul> <li><code>working</code>: Working data file</li> <li><code>master</code>: Master data file</li> </ul> required <code>path_origin</code> <code>str, pathlib.Path</code> <p>Location of the original (usually the most recent) <code>CLP</code>/<code>XLP</code> file.</p> required <code>path_destiny</code> <code>str, pathlib.Path</code> <p>Location where the new copy of the <code>CLP</code>/<code>XLP</code> file will be saved. The name of the copied file will include the current date as a prefix. For instance, the file <code>Working CLP Data.xlsx</code> will be saved as <code>YYYY-MM-DD CLP Working Data.xlsx</code>.</p> required <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>new_file_path</code> <code>pathlib.Path</code> <p>Path of the copied data file. Includes the file name and its  extension. If no original data file was found, <code>new_file_path</code> is <code>None</code>.</p> Source code in <code>kipy\\input_data.py</code> <pre><code>def copy_most_recent_data_file(product, file_type, path_origin, path_destiny, verbose=True):\n    \"\"\" `copy_most_recent_data_file`\n\n    Create a time-stamped copy of the most recent data file\n    in the required location.\n\n    Parameters\n    ----------\n    product : str\n        The product of interest. It accepts the following values:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    file_type : str\n        File type. It accepts the following values:\n\n        * `working`: Working data file\n        * `master`: Master data file\n\n    path_origin : str, pathlib.Path\n        Location of the original (usually the most recent) `CLP`/`XLP` file.\n\n    path_destiny : str, pathlib.Path\n        Location where the new copy of the `CLP`/`XLP` file will be saved.\n        The name of the copied file will include the current date as\n        a prefix. For instance, the file `Working CLP Data.xlsx` will\n        be saved as `YYYY-MM-DD CLP Working Data.xlsx`.\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    new_file_path : pathlib.Path\n        Path of the copied data file. Includes the file name and its \n        extension. If no original data file was found, `new_file_path`\n        is `None`.\n\n    \"\"\"\n\n    # Ensure that paths are pathlib.Paths.\n    path_origin = pathlib.Path(path_origin)\n    path_destiny = pathlib.Path(path_destiny)\n\n    # Convert inputs for ease of use.\n    if (product == 'yescarta') or (product=='clp'):\n        product = 'clp'\n    elif product == 'tecartus' or (product=='xlp'):\n        product = 'xlp'\n    else:\n        raise Exception(f\"Product {product} is not valid.\")\n\n\n    # Cycle through all the data files in path origin until a match is found.\n    # If no match is found, file_interest will be None.\n    # Notice that we will only catch the first file that is found.\n    # In the current file structure, this works fine, but if that changes,\n    # this function might need to be adapted.\n    file_interest = None\n\n    for file in path_origin.rglob('*.xlsx'):\n\n        # Convert to string in lower case.\n        file_name = str(file.stem).lower()\n\n        if (product in file_name) &amp; (file_type in file_name):\n            file_interest = file\n            if verbose:\n                print(f\"File {file_interest} found!\")\n            break\n\n    if file_interest is None:\n        if verbose:\n            print(f\"No file found in {path_origin} that matches product {product} and type {file_type}\")\n        new_file_path = None\n    else:\n        date_today = datetime.today().strftime('%Y-%m-%d')\n        new_file_name = f'{date_today} {product.upper()} {file_type.capitalize()} Data.xlsx'\n\n        # Ensure that path_destiny exists.\n        if not path_destiny.exists():\n            if verbose:\n                print(f\"\\tDirectory {path_destiny} does not exist. Creating...\", flush=False, end='')\n            path_destiny.mkdir(parents=True)\n            if verbose:\n                print(\"\\tDONE!\")\n\n        new_file_path = pathlib.Path(path_destiny, new_file_name)\n\n        if verbose:\n            print(f\"Copying file from {file_interest} to {new_file_path}...\", flush=True, end=\"\")\n        shutil.copyfile(file_interest, new_file_path)\n        if verbose:\n            print(\"\\tDONE!\")\n\n    return new_file_path\n</code></pre> <p><code>read_excel_file_raw</code></p> <p>Read a raw <code>CLP</code>/<code>XLP</code> Excel data file.</p> <p>Warning</p> <p>Notice that there's a different behaviour when reading from a <code>.csv</code>  compared to when reading from a <code>.xlsx</code> (i.e., Excel) file. For example,  when reading from a <code>.csv</code> file, columns that end with a percentage  will be read as strings (e.g., <code>'75%'</code>); when reading from an Excel file,  the same columns will be read as floats (e.g., <code>0.75</code>).</p> <p>Parameters:</p> Name Type Description Default <code>path_to_file</code> <code>str, pathlib.Path</code> <p>Path to the desired <code>CLP</code> or <code>XLP</code> Excel data file.  Should include the extension (e.g., <code>filename.xlsx</code>).</p> required <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df</code> <code>pandas DataFrame</code> <p>Raw DataFrame.</p> Source code in <code>kipy\\input_data.py</code> <pre><code>def read_excel_file_raw(path_to_file, verbose=True):\n    \"\"\"`read_excel_file_raw`\n\n    Read a raw `CLP`/`XLP` Excel data file.\n\n    !!! warning\n\n        Notice that there's a different behaviour when reading from a `.csv` \n        compared to when reading from a `.xlsx` (i.e., Excel) file. For example, \n        when reading from a `.csv` file, columns that end with a percentage \n        will be read as strings (e.g., `'75%'`); when reading from an Excel file, \n        the same columns will be read as floats (e.g., `0.75`).\n\n    Parameters\n    ----------\n    path_to_file : str, pathlib.Path\n        Path to the desired `CLP` or `XLP` Excel data file. \n        Should include the extension (e.g., `filename.xlsx`).\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df: pandas DataFrame\n        Raw DataFrame.  \n    \"\"\"\n\n    # Make sure that path_to_file is a pathlib.Path\n    path_to_file = pathlib.Path(path_to_file)\n    file_name = path_to_file.name\n\n\n    # CLP (YESCARTA)\n    if ('clp' in str(file_name).lower()) or ('yescarta' in str(file_name).lower()):\n        if verbose:\n            print(f\"Reading YESCARTA file {file_name}... \", end='', flush=True)\n\n        df = pd.read_excel(path_to_file, sheet_name=0)\n\n        if verbose:\n            print(\"\\tDONE!\")\n\n\n    # XLP (TECARTUS)\n    # The XLP file has an additional row on top. Thus, the header = 1.\n    elif ('xlp' in str(file_name).lower()) or ('tecartus' in str(file_name).lower()):\n        if verbose:        \n            print(f\"Reading TECARTUS {file_name}... \", end='', flush=True)\n\n        df = pd.read_excel(path_to_file, sheet_name=0, header=1)\n\n        if verbose:\n            print(\"\\tDONE!\")\n\n\n    # Unknown file.\n    else:\n        print(\"File {file_name} doesn't specify if CLP or XLP. Will be read with header=0\")\n\n        if verbose:        \n            print(f\"Reading unknown {file_name}... \", end='', flush=True)\n\n        df = pd.read_excel(path_to_file, sheet_name=0)\n\n        if verbose:\n            print(\"\\tDONE!\")\n\n    return df\n</code></pre>"},{"location":"api_reference/kpis/","title":"KPIs","text":"<p>These functions are used for calculating KPIs.</p> <p><code>calculate_kpis</code></p> <p>Calculate basic set of KPIs.</p> <p>Info</p> <p>Additional KPIs are calculated in the function <code>kipy.calculate_kpis2</code></p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame with deviation information. In other words,  a DataFrame that passed already through the following functions:</p> <ul> <li> <p>kipy.classify_deviation</p> </li> <li> <p>kipy.classify_deviation_extra</p> </li> <li> <p>kipy.count_deviations</p> </li> </ul> required <code>verbose</code> <code>bool, optional</code> <p>Define if verbose output will be printed (True) or not (False).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_kpis</code> <code>pandas DataFrame</code> <p>A DataFrame with the following KPIs:</p> KPI Name Description <code>n_batches</code> Number of batches <code>harvested_n</code> Number of harvested batches <code>harvested_p</code> Percentage of harvested batches <code>terminated_n</code> Number of terminated batches <code>terminated_p</code> Percentage of terminated batches <code>oos_n</code> Number of batches with OOS <code>oos_p</code> Percentage of batches with OOS <code>no_oos_n</code> Number of batches without OOS <code>no_oos_p</code> Percentage of batches without OOS <p><code>calculate_kpis2</code></p> <p>Calculate KPIs of interest in past and present.</p> <p>Info</p> <p>Basic KPIs are calculated in the function <code>kipy.calculate_kpis</code></p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame with deviation information. In other words,  a DataFrame that passed already through the following functions:</p> <ul> <li> <p>kipy.classify_deviation</p> </li> <li> <p>kipy.classify_deviation_extra</p> </li> <li> <p>kipy.count_deviations</p> </li> </ul> required <code>verbose</code> <code>bool, optional</code> <p>Define if verbose output will be printed (True) or not (False).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_kpis</code> <code>pandas DataFrame</code> <p>A DataFrame with the following KPIs:</p> KPI Name Description <code>X_batches_n</code> Number of batches <code>X_fresh_n</code> Number of fresh batches <code>X_fresh_p</code> Percentage of fresh batches <code>X_frozen_n</code> Number of frozen batches <code>X_frozen_p</code> Percentage of frozen batches <code>X_terminated_n</code> Number of terminated batches (based on their status) <code>X_terminated_p</code> Percentage of terminated batches (based on their status) <code>X_oos_rejected_n</code> Number of batches with status \"rejected\" or \"pending\" with OOS <code>X_oos_rejected_p</code> Percentage of batches with status \"rejected\" or \"pending\" with OOS <code>X_oos_physicians_n</code> Number of batches with status \"physician's release\" with OOS <code>X_oos_physicians_p</code> Percentage of batches with status \"physician's release\" with OOS <code>X_bags_n</code> Number of produced bags <code>X_bags_vi_n</code> Number of bags with a VI value of <code>True</code> <code>X_bags_vi_p</code> Percentage of bags with a VI value of <code>True</code> <code>X_oos_batches_n</code> Number of batches with OOS <code>X_oos_batches_p</code> Percentage of batches with OOS <code>X_success_n</code> Number of batches that were successful <code>X_success_p</code> Percentage of batches that were successful <code>X_success_fresh_n</code> Number of fresh batches that were successful <code>X_success_fresh_p</code> Percentage of fresh batches that were successful <code>X_success_frozen_n</code> Number of frozen batches that were successful <code>X_success_frozen_p</code> Percentage of frozen batches that were successful <p>where X can either be <code>past</code> or <code>present</code>.</p>"},{"location":"api_reference/kpis/#kipy.calculate_kpis--see-also","title":"See also","text":"<ul> <li><code>kipy.calculate_kpis2</code></li> </ul> Source code in <code>kipy\\kpis.py</code> <pre><code>def calculate_kpis(df, verbose=True):\n    \"\"\"`calculate_kpis`\n\n    Calculate basic set of KPIs.\n\n    !!! info\n        Additional KPIs are calculated in the function [`kipy.calculate_kpis2`][]\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame with deviation information. In other words, \n        a DataFrame that passed already through the following functions:\n\n        * [kipy.classify_deviation][]\n\n        * [kipy.classify_deviation_extra][]\n\n        * [kipy.count_deviations][]\n\n    verbose : bool, optional\n        Define if verbose output will be printed (True) or not (False).\n\n    Returns\n    -------\n    df_kpis : pandas DataFrame\n        A DataFrame with the following KPIs:\n\n        | KPI Name       | Description                         |\n        |----------------|-------------------------------------|\n        | `n_batches`    | Number of batches                   |\n        | `harvested_n`  | Number of harvested batches         |\n        | `harvested_p`  | Percentage of harvested batches     |\n        | `terminated_n` | Number of terminated batches        |\n        | `terminated_p` | Percentage of terminated batches    |\n        | `oos_n`        | Number of batches with OOS          |\n        | `oos_p`        | Percentage of batches with OOS      |\n        | `no_oos_n`     | Number of batches *without* OOS     |\n        | `no_oos_p`     | Percentage of batches *without* OOS |\n\n    See also\n    --------\n    * [`kipy.calculate_kpis2`][]\n    \"\"\"\n    if verbose:\n        print(\"Calculating set of KPIs...\", end='', flush=True)\n\n    kpis = {}\n\n    kpis['n_batches'] = len(df)\n    kpis['harvested_n'] = len(df.query('harvested == True'))\n    kpis['harvested_p'] = round((kpis['harvested_n'] / kpis['n_batches']) * 100, 2)\n    kpis['terminated_n'] = len(df.query('harvested == False'))\n    kpis['terminated_p'] = round((kpis['terminated_n'] / kpis['n_batches']) * 100, 2)\n    kpis['oos_n'] = len(df.query('oos == True'))\n    kpis['oos_p'] = round((kpis['oos_n'] / kpis['n_batches']) * 100, 2)\n    kpis['no_oos_n'] = kpis['n_batches'] - kpis['oos_n']\n    kpis['no_oos_p'] = round((kpis['no_oos_n'] / kpis['n_batches']) * 100, 2)\n\n    df_kpis = pd.DataFrame(kpis.items(), columns=['kpi', 'value'])\n\n    if verbose:\n        print(\"\\tDONE!\")\n\n    return df_kpis\n</code></pre>"},{"location":"api_reference/kpis/#kipy.calculate_kpis2--see-also","title":"See also","text":"<ul> <li><code>kipy.calculate_kpis</code></li> </ul> Source code in <code>kipy\\kpis.py</code> <pre><code>def calculate_kpis2(df, date_end, verbose=True):\n    \"\"\"`calculate_kpis2`\n\n    Calculate KPIs of interest in past and present.\n\n    !!! info\n        Basic KPIs are calculated in the function [`kipy.calculate_kpis`][]\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame with deviation information. In other words, \n        a DataFrame that passed already through the following functions:\n\n        * [kipy.classify_deviation][]\n\n        * [kipy.classify_deviation_extra][]\n\n        * [kipy.count_deviations][]\n\n    verbose : bool, optional\n        Define if verbose output will be printed (True) or not (False).\n\n    Returns\n    -------\n    df_kpis : pandas DataFrame\n        A DataFrame with the following KPIs:\n\n        | KPI Name             | Description                                                        |\n        |----------------------|--------------------------------------------------------------------|\n        | `X_batches_n`        | Number of batches                                                  |\n        | `X_fresh_n`          | Number of fresh batches                                            |\n        | `X_fresh_p`          | Percentage of fresh batches                                        |\n        | `X_frozen_n`         | Number of frozen batches                                           |\n        | `X_frozen_p`         | Percentage of frozen batches                                       |\n        | `X_terminated_n`     | Number of terminated batches (based on their status)               |\n        | `X_terminated_p`     | Percentage of terminated batches (based on their status)           |\n        | `X_oos_rejected_n`   | Number of batches with status \"rejected\" or \"pending\" with OOS     |\n        | `X_oos_rejected_p`   | Percentage of batches with status \"rejected\" or \"pending\" with OOS |\n        | `X_oos_physicians_n` | Number of batches with status \"physician's release\" with OOS       |\n        | `X_oos_physicians_p` | Percentage of batches with status \"physician's release\" with OOS   |\n        | `X_bags_n`           | Number of produced bags                                            |\n        | `X_bags_vi_n`        | Number of bags with a VI value of `True`                           |\n        | `X_bags_vi_p`        | Percentage of bags with a VI value of `True`                       |\n        | `X_oos_batches_n`    | Number of batches with OOS                                         |\n        | `X_oos_batches_p`    | Percentage of batches with OOS                                     |\n        | `X_success_n`        | Number of batches that were successful                             |\n        | `X_success_p`        | Percentage of batches that were successful                         |\n        | `X_success_fresh_n`  | Number of *fresh* batches that were successful                     |\n        | `X_success_fresh_p`  | Percentage of *fresh* batches that were successful                 |\n        | `X_success_frozen_n` | Number of *frozen* batches that were successful                    |\n        | `X_success_frozen_p` | Percentage of *frozen* batches that were successful                |\n\n        where X can either be `past` or `present`.\n\n    See also\n    --------\n    * [`kipy.calculate_kpis`][]\n    \"\"\"\n\n    if verbose:\n        print(\"Calculating set of past/present KPIs (KPIs 2)...\")\n\n\n    date_start, date_partition = misc.get_date_start_partition(date_end)\n    if verbose:\n        print(f\"\\tBeginning date: {date_start}\")\n        print(f\"\\tPartition date: {date_partition}\")\n        print(f\"\\tEnd date: {date_end}\")\n\n    # Divide data into past (i.e., historical) and present.\n    if '-q' in str(date_end):\n        df_past = df.query('(production_date_q &gt;= @date_start) and (production_date_q &lt;= @date_partition)').copy()\n        df_present = df.query('production_date_q &gt; @date_partition').copy()\n    else:\n        df_past = df.query('(production_date_short_formatted &gt;= @date_start) &amp; (production_date_short_formatted &lt;= @date_partition)').copy()\n        df_present = df.query('production_date_short_formatted &gt; @date_partition').copy()\n\n\n    kpis2 = {}\n\n    # Loop, since calculations are done exactly the same for past and\n    # present data.\n    for df_, prefix in zip([df_past, df_present], ['past', 'present']):\n\n        # Calculate the number of periods.\n        # For past data, this is used to calculate the average.\n        # For present data, this will always be one (we are looking at only\n        # one period). However, we do it calculate it to keep it consistent\n        # between both cases.\n        if '-q' in str(date_end):\n            periods = list(df_['production_date_q'].unique())\n            n_periods = len(periods)\n        else:\n            periods = df_['production_date_short'].unique()\n            n_periods = len(periods)\n\n        # Initialize lists. \n        # Each element will correspond to the value of one period.\n        n_batches = []\n        n_batches_fresh = []\n        p_batches_fresh = []\n        n_batches_frozen = []\n        p_batches_frozen = []\n        n_batches_terminated = []\n        p_batches_terminated = []\n        n_oos_rejected = []\n        p_oos_rejected = []\n        n_oos_physicians = []\n        p_oos_physicians = []\n        n_bags = []\n        n_bags_vi = []\n        p_bags_vi = []\n        n_oos = []\n        p_oos = []\n        n_success = []\n        p_success = []\n        n_success_fresh = []\n        p_success_fresh = []\n        n_success_frozen = []\n        p_success_frozen = []\n\n        for period in periods:\n\n            if '-q' in str(date_end):\n                df_period = df_.query('production_date_q == @period').copy()\n            else:\n                df_period = df_.query('production_date_short == @period').copy()\n\n            # Number of total batches.\n            n_batches_period = len(df_period)\n            n_batches.append(n_batches_period)\n\n            # Number of fresh batches.\n            n_batches_fresh_period = len(df_period.query('apheresis_type == \"fresh\"'))\n            n_batches_fresh.append(n_batches_fresh_period)\n            p_batches_fresh_period = round((n_batches_fresh_period/n_batches_period)*100, 2)\n            p_batches_fresh.append(p_batches_fresh_period)\n\n            # Number of frozen batches.\n            n_batches_frozen_period = len(df_period.query('apheresis_type == \"frozen\"'))\n            n_batches_frozen.append(n_batches_frozen_period)\n            p_batches_frozen_period = round((n_batches_frozen_period/n_batches_period)*100, 2)\n            p_batches_frozen.append(p_batches_frozen_period)\n\n            # Number of terminated batches.\n            n_batches_terminated_period = len(df_period.query('status == \"terminated\"'))\n            n_batches_terminated.append(n_batches_terminated_period)\n            p_batches_terminated_period = round((n_batches_terminated_period/n_batches_period)*100, 2)\n            p_batches_terminated.append(p_batches_terminated_period)\n\n            # Number of rejected batches (with status either \"rejected\" or \"pending\") with OOS.\n            n_oos_rejected_period = len(df_period.query('(oos == True) &amp; ((status == \"rejected\") or (status == \"pending\"))'))    \n            n_oos_rejected.append(n_oos_rejected_period)\n            p_oos_rejected_period = round((n_oos_rejected_period/n_batches_period)*100, 2)\n            p_oos_rejected.append(p_oos_rejected_period)\n\n            # Number of batches with status \"physician's release\" with OOS.\n            n_oos_physicians_period = len(df_period.query('(oos == True) &amp; (status == \"physician\\'s release\")'))    \n            n_oos_physicians.append(n_oos_physicians_period)\n            p_oos_physicians_period = round((n_oos_physicians_period/n_batches_period)*100, 2)\n            p_oos_physicians.append(p_oos_physicians_period)\n\n            # Number of bags\n            df_bags = df_period[['vi_bag1_deviation', 'vi_bag2_deviation']]\n            n_bags_period = (len(df_bags) * 2) - df_bags.isnull().sum().sum()\n            n_bags.append(n_bags_period)\n\n            # Number of bags with VI.\n            n_bags_vi_period = df_bags.sum().sum()\n            n_bags_vi.append(n_bags_vi_period)\n            p_bags_vi_period = round((n_bags_vi_period/n_bags_period)*100, 2)\n            p_bags_vi.append(p_bags_vi_period)\n\n            # Number of batches with OOS.\n            n_oos_period = len(df_period.query('oos == True'))\n            n_oos.append(n_oos_period)\n            p_oos_period = round((n_oos_period/n_batches_period)*100, 2)\n            p_oos.append(p_oos_period)\n\n            # Success rate.\n            n_success_period = len(df_period.query('success == True'))\n            n_success.append(n_success_period)\n            p_success_period = round((n_success_period/n_batches_period)*100, 2)\n            p_success.append(p_success_period)            \n\n            # Success rate of fresh batches.\n            n_success_fresh_period = len(df_period.query('(success == True) &amp; (apheresis_type == \"fresh\")'))\n            n_success_fresh.append(n_success_fresh_period)            \n            p_success_fresh_period = round((n_success_fresh_period/n_batches_period)*100, 2)\n            p_success_fresh.append(p_success_fresh_period)  \n\n            # Success rate of frozen batches.\n            n_success_frozen_period = len(df_period.query('(success == True) &amp; (apheresis_type == \"frozen\")'))\n            n_success_frozen.append(n_success_frozen_period)\n            p_success_frozen_period = round((n_success_frozen_period/n_batches_period)*100, 2)\n            p_success_frozen.append(p_success_frozen_period) \n\n        if n_periods &gt; 0:\n            kpis2[prefix + '_batches_n'] = round(sum(n_batches)/n_periods, 2)\n            kpis2[prefix + '_fresh_n'] = round(sum(n_batches_fresh)/n_periods, 2)\n            kpis2[prefix + '_fresh_p'] = round(sum(p_batches_fresh)/n_periods, 2)\n            kpis2[prefix + '_frozen_n'] = round(sum(n_batches_frozen)/n_periods, 2)\n            kpis2[prefix + '_frozen_p'] = round(sum(p_batches_frozen)/n_periods, 2)\n            kpis2[prefix + '_terminated_n'] = round(sum(n_batches_terminated)/n_periods, 2)\n            kpis2[prefix + '_terminated_p'] = round(sum(p_batches_terminated)/n_periods, 2)\n            kpis2[prefix + '_oos_rejected_n'] = round(sum(n_oos_rejected)/n_periods, 2)\n            kpis2[prefix + '_oos_rejected_p'] = round(sum(p_oos_rejected)/n_periods, 2)\n            kpis2[prefix + '_oos_physicians_n'] = round(sum(n_oos_physicians)/n_periods, 2)\n            kpis2[prefix + '_oos_physicians_p'] = round(sum(p_oos_physicians)/n_periods, 2)\n            kpis2[prefix + '_bags_n'] = round(sum(n_bags)/n_periods, 2)\n            kpis2[prefix + '_bags_vi_n'] = round(sum(n_bags_vi)/n_periods, 2)\n            kpis2[prefix + '_bags_vi_p'] = round(sum(p_bags_vi)/n_periods, 2)\n            kpis2[prefix + '_oos_batches_n'] = round(sum(n_oos)/n_periods, 2)\n            kpis2[prefix + '_oos_batches_p'] = round(sum(p_oos)/n_periods, 2)\n            kpis2[prefix + '_success_n'] = round(sum(n_success)/n_periods, 2)\n            kpis2[prefix + '_success_p'] = round(sum(p_success)/n_periods, 2)\n            kpis2[prefix + '_success_fresh_n'] = round(sum(n_success_fresh)/n_periods, 2)\n            kpis2[prefix + '_success_fresh_p'] = round(sum(p_success_fresh)/n_periods, 2)\n            kpis2[prefix + '_success_frozen_n'] = round(sum(n_success_frozen)/n_periods, 2)\n            kpis2[prefix + '_success_frozen_p'] = round(sum(p_success_frozen)/n_periods, 2)\n        else:\n            kpis2[prefix + '_batches_n'] = np.nan\n            kpis2[prefix + '_fresh_n'] = np.nan\n            kpis2[prefix + '_fresh_p'] = np.nan\n            kpis2[prefix + '_frozen_n'] = np.nan\n            kpis2[prefix + '_frozen_p'] = np.nan\n            kpis2[prefix + '_terminated_n'] = np.nan\n            kpis2[prefix + '_terminated_p'] = np.nan\n            kpis2[prefix + '_oos_rejected_n'] = np.nan\n            kpis2[prefix + '_oos_rejected_p'] = np.nan\n            kpis2[prefix + '_oos_physicians_n'] = np.nan\n            kpis2[prefix + '_oos_physicians_p'] = np.nan\n            kpis2[prefix + '_bags_n'] = np.nan\n            kpis2[prefix + '_bags_vi_n'] = np.nan\n            kpis2[prefix + '_bags_vi_p'] = np.nan\n            kpis2[prefix + '_oos_batches_n'] = np.nan\n            kpis2[prefix + '_oos_batches_p'] = np.nan\n            kpis2[prefix + '_success_n'] = np.nan\n            kpis2[prefix + '_success_p'] = np.nan\n            kpis2[prefix + '_success_fresh_n'] = np.nan\n            kpis2[prefix + '_success_fresh_p'] = np.nan\n            kpis2[prefix + '_success_frozen_n'] = np.nan\n            kpis2[prefix + '_success_frozen_p'] = np.nan\n\n    df_kpis2 = pd.DataFrame(kpis2.items(), columns=['kpi2', 'value'])\n\n    if verbose:\n        print(\"\\t\\t\\tDONE!\")\n\n    return df_kpis2\n</code></pre>"},{"location":"api_reference/miscellaneous/","title":"Miscellaneous","text":"<p>These functions provide handy tools that can be used in different places and do not belong to a specific step.</p> <p><code>get_date_end</code></p> <p>Based on the current day (i.e., the day when the script is being run),  get the end date of the data to be used.</p> <p>Example</p> Current date <code>time_resolution</code> <code>date_end</code> <code>date_end</code> format May 15, 2023 <code>monthly</code> <code>2023-04-30 23:59:59</code> pandas timestamp May 15, 2023 <code>quarter</code> <code>'2023-q1'</code> string <p>Parameters:</p> Name Type Description Default <code>time_resolution</code> <code>str</code> <p>Time resolution of the desired <code>latest_date</code>. There are two possible options:</p> <ul> <li> <p><code>'month'</code>: the end date is given as the last day of the month  previous to the day when the script is being run.</p> </li> <li> <p><code>'quarter'</code>: the end date is the quarter previous to the quarter of the day when the script is being run.</p> </li> </ul> required <p>Returns:</p> Name Type Description <code>date_end</code> <code>pandas datetime or str</code> <p>Date delimiting the data to be used.</p> <p><code>get_date_start_partition</code></p> <p>Based on the given <code>end_date</code>, get a <code>date_start</code> (i.e., the starting date of the data to be used) and a <code>date_partition</code> (i.e., the date that  partitions the date between historical data and present data).</p> <p>Parameters:</p> Name Type Description Default <code>end_date</code> <code>str</code> <p>End date of the data to be used. It has the following format:</p> <ul> <li> <p>For a <code>time_resolution</code> of <code>month</code>, <code>2023-10-31 23:59</code></p> </li> <li> <p>For a <code>time_resolution</code> of <code>quarter</code>, <code>2023-q3</code></p> </li> </ul> <p>It can be obtained using the function <code>kipy.get_date_end</code>.</p> required <code>offset</code> <code>int</code> <p>Offset to consider for defining <code>date_start</code> before <code>date_partition</code>.</p> <ul> <li> <p>If <code>end_date</code> is defined monthly, <code>offset</code> defaults to 12,  which corresponds to a year.</p> </li> <li> <p>If <code>end_date</code> is defined quarterly, <code>offset</code> defaults to 3, which corresponds to a year (due to how quarter are defined, since they don't encompass individual days).</p> </li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>date_start, date_partition</code> <p>Dates that mark the start and the partition of the data.</p> <p>Example</p> Current date Current quarter End date <code>date_start</code> <code>date_partition</code> May 15, 2023 2023-Q2 <code>2023-04-30 23:59:59</code> <code>2022-04-01 00:00:00</code> <code>2023-03-31 23:59:59</code> <code>2023-q2</code> <code>2022-q2</code> <code>2023-q1</code> <p><code>save_df_as_csv</code></p> <p>Wrapper to make sure that all <code>.csv</code> result files are saved in the same  way. </p> <p>Tip</p> <p>In case we decide to change how to save <code>.csv</code> files, we can do so  in a single place.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame to be saved.</p> required <code>file_path</code> <code>string, pathlib.Path</code> <p>Path where the file will be saved. It includes the file name.</p> required <code>sep</code> <code>string</code> <p>Character used as a column separator for the <code>.csv</code> file.</p> <p>Tip</p> <p>For Power BI dashboards, using <code>';'</code> is recommended! Make sure to configure the settings properly to align with this  too.</p> <code>';'</code> <code>decimal</code> <code>string</code> <p>Character used as decimal separator for the <code>.csv</code> file.</p> <p>Tip</p> <p>For Power BI dashboards, using <code>','</code> is recommended! Make sure to configure the settings properly to align with this  too.</p> <code>','</code> <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Type Description <code>None</code> Source code in <code>kipy\\miscellaneous.py</code> <pre><code>def save_df_as_csv(df, file_path, sep=';', decimal=',', verbose=True):\n    \"\"\"`save_df_as_csv`\n\n    Wrapper to make sure that all `.csv` result files are saved in the same \n    way. \n\n    !!! tip\n        In case we decide to change how to save `.csv` files, we can do so \n        in a single place.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be saved.\n\n    file_path : string, pathlib.Path\n        Path where the file will be saved. It includes the file name.\n\n    sep : string\n        Character used as a column separator for the `.csv` file.\n\n        !!! tip\n            For Power BI dashboards, using `';'` is recommended!\n            Make sure to configure the settings properly to align with this \n            too.\n\n    decimal : string\n        Character used as decimal separator for the `.csv` file.\n\n        !!! tip\n            For Power BI dashboards, using `','` is recommended!\n            Make sure to configure the settings properly to align with this \n            too.\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    # Ensure path is a pathlib.Path (and not just a string).\n    file_path = pathlib.Path(file_path)\n\n    # Actually save the file.\n    df.to_csv(file_path, index=False, sep=sep, decimal=decimal)\n\n    if verbose:\n        print(f\"{file_path} was saved successfully (separator = {sep}, decimal = {decimal}).\")\n\n    return None\n</code></pre> <p><code>get_elements_with_substring</code></p> <p>Get elements of a list that have a specific substring.</p> <p>Tip</p> <p>This is a handy function for process monitoring. It is used  for extracting column names of a specific type (e.g., columns that  are OOS and their names in the DataFrame end with <code>_oos</code>).</p> <p>Parameters:</p> Name Type Description Default <code>base_list</code> <code>list</code> <p>Base lists (for example, <code>list(df.columns)</code>)</p> required <code>substr_list</code> <code>list</code> <p>Each element of the list is a substring to be found. </p> <p>Tip</p> <p>If interested in only one substring, pass a list with one element.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List with the elements that have the given substring. If none, returns an empty list.</p> <p><code>group_column_in_time</code></p> <p>Group the column of a DataFrame by counting the number and percentage of batches per period (either month or quarter). It also allows to be filtered by <code>date_start</code> and <code>date_end</code>.</p> <p>Example</p> <p>If <code>col_name</code> is <code>'harvested'</code> (which has two possible values: <code>True</code> and <code>False</code>), <code>col_period</code> is <code>'production_date_month'</code>, and <code>col_year</code> is <code>'production_date_year'</code>, <code>df_final</code> look as follows:</p> <code>harvested</code> <code>production_date_month</code> <code>production_date_year</code> <code>n</code> <code>percentage</code> <code>production_date_month_name</code> <code>production_date_short</code> <code>production_date_short_formatted</code> <code>True</code> <code>01</code> <code>2022</code> <code>10</code> <code>50.00</code> <code>'January'</code> <code>'2023-01'</code> <code>2023-01-01 00:00:00</code> <code>True</code> <code>02</code> <code>2022</code> <code>15</code> <code>75.00</code> <code>'February'</code> <code>'2023-02'</code> <code>2023-02-01 00:00:00</code> <code>...</code> <code>...</code> <code>...</code> <code>...</code> <code>...</code> <code>...</code> <code>...</code> <code>...</code> <code>True</code> <code>12</code> <code>2022</code> <code>20</code> <code>100.00</code> <code>'December'</code> <code>'2023-12'</code> <code>2023-12-01 00:00:00</code> <code>False</code> <code>01</code> <code>2022</code> <code>10</code> <code>50.00</code> <code>'January'</code> <code>'2023-01'</code> <code>2023-01-01 00:00:00</code> <code>False</code> <code>02</code> <code>2022</code> <code>5</code> <code>25.00</code> <code>'February'</code> <code>'2023-02'</code> <code>2023-02-01 00:00:00</code> <code>...</code> <code>...</code> <code>...</code> <code>...</code> <code>...</code> <code>...</code> <code>...</code> <code>...</code> <code>False</code> <code>12</code> <code>2022</code> <code>0</code> <code>0.00</code> <code>'December'</code> <code>'2023-12'</code> <code>2023-12-01 00:00:00</code> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>Original DataFrame.</p> required <code>col_name</code> <code>str</code> <p>Name of the column to be grouped.</p> required <code>col_period</code> <code>str</code> <p>Name of the column in the DataFrame that has the period information. Possible values are:</p> <ul> <li> <p><code>'production_date_month '</code></p> </li> <li> <p><code>'production_date_quarter'</code></p> </li> </ul> required <code>col_year</code> <code>str</code> <p>Name of the column in the DataFrame that has the production year data.</p> <p>Info</p> <p>It defaults to <code>'production_date_year'</code> as given by the function <code>kipy.rename_columns</code> and the default data dictionary.</p> <code>'production_date_year'</code> <code>date_start</code> <code>pandas datetime (for 'month') or str (for 'quarter')</code> <p>If different from <code>None</code>, date to be used for filtering.</p> <code>None</code> <code>date_end</code> <code>pandas datetime (for 'month') or str (for 'quarter')</code> <p>If different from <code>None</code>, date to be used for filtering.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_polished</code> <code>pandas DataFrame</code> <p>DataFrame with the following columns:</p> Column name Description <code>col_name</code> Same name as <code>'col_name'</code> <code>col_period</code> Same name as <code>'col_period'</code> * If <code>production_date_month</code>, it contains months (01, 02, etc.) * If <code>production_date_quarter</code>, it contains quarte (q1, q2, etc.) <code>col_year</code> Same name as <code>'col_year'</code> <code>n</code> Number of batches for that combination <code>percentage</code> Percentage of batches for that combination (with two decimals) <code>production_date_month_name</code> Name of the production month <code>production_date_short</code> Production date (short) <code>production_date_short_formatted</code> Produciton date short (in proper format) <p><code>n</code> and <code>percentage</code> correspond to the batches in that time period.</p>"},{"location":"api_reference/miscellaneous/#kipy.get_date_end--see-also","title":"See also","text":"<ul> <li><code>kipy.get_date_start_partition</code></li> </ul> Source code in <code>kipy\\miscellaneous.py</code> <pre><code>def get_date_end(time_resolution):\n    \"\"\"`get_date_end`\n\n    Based on the current day (i.e., the day when the script is being run), \n    get the end date of the data to be used.\n\n    !!! example\n\n        | Current date | `time_resolution` | `date_end`            | `date_end` format |\n        |--------------|-------------------|-----------------------|-------------------|\n        | May 15, 2023 | `monthly`         | `2023-04-30 23:59:59` | [pandas timestamp](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html)  |\n        | May 15, 2023 | `quarter`         | `'2023-q1'`           | string            |\n\n    Parameters\n    ----------\n    time_resolution : str\n        Time resolution of the desired `latest_date`.\n        There are two possible options:\n\n        * `'month'`: the end date is given as the last day of the month \n        previous to the day when the script is being run.\n\n        * `'quarter'`: the end date is the quarter previous to the\n        quarter of the day when the script is being run.\n\n    Returns\n    -------\n    date_end : pandas datetime or str\n        Date delimiting the data to be used.\n\n    See also\n    --------\n    * [`kipy.get_date_start_partition`][]\n    \"\"\"\n\n    if time_resolution == 'month':\n        end_date = pd.to_datetime('today') - pd.offsets.MonthEnd(1)\n\n        # Make sure the latest date is at 23:59:59    \n        end_date = end_date.replace(hour=23, minute=59, second=59, microsecond=0)\n\n\n    elif time_resolution == 'quarter':\n        end_date = pd.to_datetime('today').to_period('Q') - 1\n        end_date = str(end_date.year) + '-q' + str(end_date.quarter)\n\n    else:\n        raise Exception(f\"{time_resolution} is not a valid value for time_resolution.\")\n\n\n    return end_date\n</code></pre>"},{"location":"api_reference/miscellaneous/#kipy.get_date_start_partition--see-also","title":"See also","text":"<ul> <li><code>kipy.get_date_end</code></li> </ul> Source code in <code>kipy\\miscellaneous.py</code> <pre><code>def get_date_start_partition(end_date, offset=None):\n    \"\"\"`get_date_start_partition`\n\n    Based on the given `end_date`, get a `date_start` (i.e., the starting date\n    of the data to be used) and a `date_partition` (i.e., the date that \n    partitions the date between historical data and present data).\n\n    Parameters\n    ----------\n    end_date : str\n        End date of the data to be used. It has the following format:\n\n        * For a `time_resolution` of `month`, `2023-10-31 23:59`\n\n        * For a `time_resolution` of `quarter`, `2023-q3`\n\n        It can be obtained using the function [`kipy.get_date_end`][].\n\n    offset : int\n        Offset to consider for defining `date_start` before `date_partition`.\n\n        * If `end_date` is defined monthly, `offset` defaults to 12, \n        which corresponds to a year.\n\n        * If `end_date` is defined quarterly, `offset` defaults to 3,\n        which corresponds to a year (due to how quarter are defined, since\n        they don't encompass individual days).\n\n\n    Returns\n    -------\n    date_start, date_partition : same type as date_end\n        Dates that mark the start and the partition of the data.\n\n        !!! example\n\n            | Current date | Current quarter | End date              | `date_start`          | `date_partition`      |\n            |--------------|-----------------|-----------------------|-----------------------|-----------------------|\n            | May 15, 2023 | 2023-Q2         | `2023-04-30 23:59:59` | `2022-04-01 00:00:00` | `2023-03-31 23:59:59` |\n            |              |                 | `2023-q2`             | `2022-q2`             | `2023-q1            ` |\n\n\n    See also\n    --------\n    * [`kipy.get_date_end`][]\n\n    \"\"\"\n    # The partition date allows dividing batches from the current (previous)\n    # period of interest and the past.\n    # For example, if we are are on 2023-06-XX, the latest_date will be\n    # 2023-05-31. Then, partition_date will be \n    # * 2023-04-30 for monthly\n    # * 2023-q1 for quarter\n    if '-q' in str(end_date):\n        partition_date = pd.to_datetime(end_date).to_period('Q') - 1\n        partition_date = str(partition_date.year) + '-q' + str(partition_date.quarter)\n\n        if offset is None:\n            offset = 3\n    else:\n        partition_date = end_date - pd.offsets.MonthEnd(1)\n\n        if offset is None:\n            offset = 12\n\n    # The beginning_date is 4 quarters/12 months before the partition date, since\n    # that is the time period that we are using as a reference now.\n    # We subtract115 months and go back to the begining of that month,\n    # making it 12 effectively. For example, if partition date is 2023-04-30, \n    # then beginnning_date will be \n    # * 2022-05-01 for monthly\n    # * 2022-q1 for quarter\n    if '-q' in str(end_date):\n        beginning_date = pd.to_datetime(partition_date).to_period('Q') - offset\n        beginning_date = str(beginning_date.year) + '-q' + str(beginning_date.quarter)\n    else:\n        beginning_date = end_date - pd.offsets.MonthEnd(offset)\n        beginning_date = beginning_date.replace(day=1, hour=00, minute=00, second=00, microsecond=0)\n\n    return beginning_date, partition_date\n</code></pre>"},{"location":"api_reference/miscellaneous/#kipy.get_elements_with_substring--references","title":"References","text":"<ul> <li>Filtering a list of strings based on a substring</li> </ul> Source code in <code>kipy\\miscellaneous.py</code> <pre><code>def get_elements_with_substring(base_list, substr_list):\n    \"\"\" `get_elements_with_substring`\n\n    Get elements of a list that have a specific substring.\n\n    !!! tip\n        This is a handy function for process monitoring. It is used \n        for extracting column names of a specific type (e.g., columns that \n        are OOS and their names in the DataFrame end with `_oos`).\n\n    Parameters\n    ----------\n    base_list : list\n        Base lists (for example, `list(df.columns)`)\n\n    substr_list : list\n        Each element of the list is a substring to be found. \n\n        !!! tip\n            If interested in only one substring, pass a list with one element.\n\n    Returns\n    -------\n    list\n        List with the elements that have the given substring.\n        If none, returns an empty list.\n\n    References\n    ----------\n    * [Filtering a list of strings based on a substring](https://www.geeksforgeeks.org/python-filter-list-of-strings-based-on-the-substring-list/)\n    \"\"\"\n\n    return [str for str in base_list if\n             any(sub in str for sub in substr_list)]\n</code></pre>"},{"location":"api_reference/miscellaneous/#kipy.group_column_in_time--references","title":"References","text":"<ul> <li>pandas value_counts (from v. 1.1)</li> </ul> Source code in <code>kipy\\miscellaneous.py</code> <pre><code>def group_column_in_time(df, col_name, col_period, col_year='production_date_year', date_start=None, date_end=None, verbose=True):\n    \"\"\"`group_column_in_time`\n\n    Group the column of a DataFrame by counting the number and percentage\n    of batches per period (either month or quarter). It also allows to be\n    filtered by `date_start` and `date_end`.\n\n    !!! example\n\n        If `col_name` is `'harvested'` (which has two possible values:\n        `True` and `False`), `col_period` is `'production_date_month'`,\n        and `col_year` is `'production_date_year'`, `df_final` look as follows:\n\n        | `harvested` | `production_date_month` | `production_date_year` | `n`   | `percentage` | `production_date_month_name` | `production_date_short` | `production_date_short_formatted ` |\n        |-------------|-------------------------|------------------------|-------|--------------|------------------------------|-------------------------|------------------------------------|\n        | `True`      | `01`                    | `2022`                 | `10`  | `50.00`      | `'January'`                  | `'2023-01'`             | `2023-01-01 00:00:00`              |\n        | `True`      | `02`                    | `2022`                 | `15`  | `75.00`      | `'February'`                 | `'2023-02'`             | `2023-02-01 00:00:00`              |\n        | `...`       | `...`                   | `...`                  | `...` | `...`        | `...`                        | `...`                   | `...`                              |\n        | `True`      | `12`                    | `2022`                 | `20`  | `100.00`     | `'December'`                 | `'2023-12'`             | `2023-12-01 00:00:00`              |\n        | `False`     | `01`                    | `2022`                 | `10`  | `50.00`      | `'January'`                  | `'2023-01'`             | `2023-01-01 00:00:00`              |\n        | `False`     | `02`                    | `2022`                 | `5`   | `25.00`      | `'February'`                 | `'2023-02'`             | `2023-02-01 00:00:00`              |\n        | `...`       | `...`                   | `...`                  | `...` | `...`        | `...`                        | `...`                   | `...`                              |\n        | `False`     | `12`                    | `2022`                 | `0`   | `0.00`       | `'December'`                 | `'2023-12'`             | `2023-12-01 00:00:00`              |\n\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        Original DataFrame.\n\n    col_name : str\n        Name of the column to be grouped.\n\n    col_period : str\n        Name of the column in the DataFrame that has the period information.\n        Possible values are:\n\n            * `'production_date_month '`\n\n            * `'production_date_quarter'`\n\n    col_year : str\n        Name of the column in the DataFrame that has the production year data.\n\n        !!! info\n            It defaults to `'production_date_year'` as given by the\n            function [`kipy.rename_columns`][] and the default data dictionary.\n\n    date_start : pandas datetime (for 'month') or str (for 'quarter')\n        If different from `None`, date to be used for filtering.\n\n    date_end : pandas datetime (for 'month') or str (for 'quarter')\n        If different from `None`, date to be used for filtering.\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df_polished : pandas DataFrame\n        DataFrame with the following columns:\n\n        | Column name                       | Description                                                       |\n        |-----------------------------------|-------------------------------------------------------------------|\n        | `col_name`                        | Same name as `'col_name'`                                         |\n        | `col_period`                      | Same name as `'col_period'`                                       |\n        |                                   | * If `production_date_month`, it contains months (01, 02, etc.)   |\n        |                                   | * If `production_date_quarter`, it contains quarte (q1, q2, etc.) |\n        | `col_year`                        | Same name as `'col_year'`                                         |\n        | `n`                               | Number of batches for that combination                            |\n        | `percentage`                      | Percentage of batches for that combination (with two decimals)    |\n        | `production_date_month_name`      | Name of the production month                                      |\n        | `production_date_short`           | Production date (short)                                           |\n        | `production_date_short_formatted` | Produciton date short (in proper format)                          |\n\n        `n` and `percentage` correspond to the batches in that time period.\n\n    References\n    ----------\n    * [pandas value_counts](https://pandas.pydata.org/pandas-docs/dev/reference/api/pandas.DataFrame.value_counts.html#pandas.DataFrame.value_counts) (from v. 1.1)\n    \"\"\"\n\n    if verbose:\n        print(f\"Grouping DataFrame by column `{col_name}` and period `{col_period}`...\", end=\"\", flush=True)\n\n    # First, we will group and count.\n    # Notice how we reset the index to make sure that we have a proper DataFrame.\n    series_grouped = df.value_counts(subset=[col_name, col_period, col_year]) \n    df_grouped = pd.DataFrame({'n': series_grouped}).reset_index()\n\n    # Additionally, we will calculate the percentage of events (and not just\n    # the absolute number) to make our life easier in Power BI.\n    df_grouped['n_percentage'] = df_grouped['n']/df_grouped.groupby([col_period, col_year])['n'].transform('sum')\n    df_grouped['n_percentage'] = round(df_grouped['n_percentage'] * 100, 2)\n\n\n    # Padding\n    # Finally, we will create a padded data frame to ensure that all \n    # possible combinations (column values, periods, years) have a value \n    # (even if 0).\n    col_values_unique = sorted(df_grouped[col_name].unique())\n    if '_q' in col_period:\n        periods_unique = ['1', '2', '3', '4']\n    elif '_m' in col_period:\n        periods_unique = ['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12']\n    years_unique = sorted(df_grouped[col_year].unique())\n\n    possible_combinations = list(itertools.product(col_values_unique, periods_unique, years_unique))\n    df_padded = pd.DataFrame(possible_combinations, columns=[col_name, col_period, col_year]) # Rename columns\n\n    # Fill in the padded data frame (missing values will be replaced by 0).\n    df_filled = df_padded.merge(df_grouped, how='outer', on=[col_name, col_period, col_year])\n    df_filled.fillna(value=0, inplace=True)\n\n\n    # Polish DataFrame for use in Power BI\n    df_polished = df_filled.copy()\n\n    # For quarter, add column `production_date_q` (e.g., 2023-q2)\n    if '_q' in col_period:\n        df_polished['production_date_q'] = df_polished[col_year] + '-q' + df_polished[col_period]\n\n    # For month, add column `production_date_month_name` (e.g., May)\n    elif '_m' in col_period:\n\n        def _month_number_to_name(row):\n            if pd.isna(row['production_date_month']):\n                month_name = '-'\n            else:\n                month_name = calendar.month_name[int(row['production_date_month'])]\n            return month_name\n\n        df_polished['production_date_month_name'] = df_polished.apply(_month_number_to_name, axis=1)\n\n\n\n    # Create production_date_short column only in the month case.\n    if '_m' in col_period:\n        df_polished['production_date_short'] = df_polished[col_year] + '-' + df_polished[col_period]\n        df_polished['production_date_short_formatted'] = pd.to_datetime(df_polished['production_date_short'], format='%Y-%m')\n\n    # Filter data to make sure that it contains batches only in the \n    # time period of interest.\n    if '_q' in col_period:\n        if date_start:\n            df_polished = df_polished.query('production_date_q &gt;= @date_start').copy()\n        if date_end:\n            df_polished = df_polished.query('production_date_q &lt;= @date_end').copy()\n\n    elif '_m' in col_period:\n        if date_start:\n            df_polished = df_polished.query('production_date_short_formatted &gt;= @date_start').copy()\n        if date_end:\n            df_polished = df_polished.query('production_date_short_formatted &lt;= @date_end').copy()\n\n    if verbose:\n        print(\"\\tDONE!\")\n\n    return df_polished\n</code></pre>"},{"location":"api_reference/preprocessing/","title":"Pre-processing","text":"<p>These functions perform pre-processing steps on the raw data.</p> <p><code>preprocess_raw_dataframe</code></p> <p>Pre-process a raw <code>CLP</code>/<code>XLP</code> DataFrame by applying the next steps:</p> <ul> <li>Rename columns (according to the given data dictionary mapping)</li> <li>Cleaning of numerical and categorical columns</li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>Raw DataFrame (as read using <code>kipy.read_excel_file_raw</code>)</p> required <code>product</code> <code>str</code> <p>The product of interest. It accepts the following values:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <code>path_dictionary</code> <code>str, pathlib.Path</code> <p>Location of the <code>.xlsx</code> data dictionary.</p> required <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_preprocessed</code> <code>pandas DataFrame</code> <p>Pre-processed DataFrame.</p> <p><code>rename_columns</code></p> <p>Rename the columns so that they have computer names as defined in the corresponding data dictionary.</p> <p>Tip</p> <p>Renaming the columns should always be the first pre-processing step.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>The pandas DataFrame.</p> required <code>product</code> <code>str</code> <p>The product of interest. It accepts the following values:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <code>path_dictionary</code> <code>str, pathlib.Path</code> <p>Location of the <code>.xlsx</code> data dictionary.</p> required <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_renamed</code> <code>pandas DataFrame</code> <p>Same as input <code>df</code>, but with the columns renamed according to the data dictionary mapping.</p> Source code in <code>kipy\\preprocessing.py</code> <pre><code>def rename_columns(df, product, path_dictionary, verbose=True):\n    \"\"\"`rename_columns`\n\n    Rename the columns so that they have computer names as defined\n    in the corresponding data dictionary.\n\n    !!! tip\n        Renaming the columns should always be the first pre-processing step.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        The pandas DataFrame.\n\n    product : str\n        The product of interest. It accepts the following values:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    path_dictionary : str, pathlib.Path\n        Location of the `.xlsx` data dictionary.\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df_renamed : pandas DataFrame\n        Same as input `df`, but with the columns renamed according\n        to the data dictionary mapping.\n    \"\"\"\n    if verbose:\n        print(\"Renaming columns...\", flush=True, end='')\n\n    # Make sure that the given path is a pathlib.Path.\n    path_dictionary = pathlib.Path(path_dictionary)\n\n    # Check that data dictionary exists.\n    if not path_dictionary.exists():\n        raise Exception(f\"Data dictionary {path_dictionary} does not exist.\")\n\n    # Read the dictionary.\n    df_dictionary = pd.read_excel(path_dictionary, \n                                  sheet_name='dictionary', \n                                  header=0)\n\n    # Select the columns of interest.\n    df_dictionary = df_dictionary[['Column Name', 'Computer Column Name']]\n\n    # Convert DataFrame to dictionary.  \n    df_dictionary_dict = dict(zip(df_dictionary['Column Name'].values, \n                                  df_dictionary['Computer Column Name'].values))\n\n    # Perform the renaming.\n    df_renamed = df.rename(columns=df_dictionary_dict)\n\n    if verbose:\n        print(\"\\tDONE!\")\n\n    return df_renamed\n</code></pre> <p><code>clean_dataframe</code></p> <p>Clean categorical and numerical columns of a <code>CLP</code>/<code>XLP</code> DataFrame.</p> <p>Info</p> <p>To identify them, it uses information from the given data dictionary. Numerical columns are those that have a <code>Type</code> of <code>int</code>, <code>float</code>, or <code>int (scientific notation)</code>. Categorical columns are those that have a  <code>Type</code> of <code>string</code>. Columns that fall outside of these types remain unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>The pandas DataFrame.</p> required <code>product</code> <code>str</code> <p>The product of interest. It accepts the following values:</p> <ul> <li><code>yescarta</code> or <code>clp</code></li> <li><code>tecartus</code> or <code>xlp</code></li> </ul> required <code>path_dictionary</code> <code>str, pathlib.Path</code> <p>Location of the <code>.xlsx</code> data dictionary.</p> required <code>cols</code> <code>list of str</code> <p>List with the columns to be cleaned. If <code>None</code>,  all columns will be (attempted to be) cleaned.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>df_clean</code> <code>pandas DataFrame</code> <p>Clean DataFrame.</p> <p><code>clean_column_numerical</code></p> <p>Clean a numerical column. It applies the following steps:</p> <ul> <li>Convert empty spaces (i.e., <code>' '</code>) to <code>NaN</code>s.</li> <li>Convert entries with a value of <code>'No Result'</code> to <code>NaN</code>s.</li> <li>Convert weird entries with a value of <code>\\xa0</code> to <code>NaN</code>s.</li> <li>Convert entries with a value of <code>'nan'</code> to <code>NaN</code>s.</li> <li>Convert <code>LOQ</code>s (i.e., entries that have a <code>&lt;</code> such as <code>&lt;LOQ</code> or <code>&lt;8</code>)    to <code>-1</code>.</li> <li>Convert entries with a value of <code>#VALUE!</code> or <code>#DIV/0!</code> to <code>NaN</code>s.</li> <li>Remove the percentage character.</li> <li>Cast to float to ensure that values will be numbers.</li> </ul> <p>Warning</p> <p>Notice that percentage entries are cleaned by removing the <code>%</code> only. For example, <code>75%</code> is cleaned as <code>0.75</code>. If an output of <code>75</code> is needed, it needs to be multiplied by <code>100</code> outside of this function (for example, as done in <code>kipy.clean_dataframe</code>)</p> <p>Danger</p> <p>If the <code>LOQ</code> definition changes in the future, this function needs  to be updated.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>The pandas DataFrame.s</p> required <code>col</code> <code>string</code> <p>Name of the numerical column to be cleaned.</p> required <p>Returns:</p> Name Type Description <code>col_clean</code> <code>pandas Series</code> <p>The clean (numerical) column.</p> <p><code>clean_column_categorical</code></p> <p>Clean a categorical column. It applies the following steps:</p> <ul> <li>Make strings lower case</li> <li>Remove leading spaces</li> <li>Remove trailing spaces</li> <li>Convert weird entries with a value of <code>\\xa0</code> to <code>NaN</code>s.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>The pandas DataFrame.</p> required <code>col</code> <code>string</code> <p>Name of the numerical column to be cleaned.</p> required <p>Returns:</p> Name Type Description <code>col_clean</code> <code>pandas Series</code> <p>The clean (numerical) column.</p>"},{"location":"api_reference/preprocessing/#kipy.preprocess_raw_dataframe--see-also","title":"See also","text":"<ul> <li><code>kipy.rename_columns</code></li> <li><code>kipy.clean_dataframe</code></li> </ul> Source code in <code>kipy\\preprocessing.py</code> <pre><code>def preprocess_raw_dataframe(df, product, path_dictionary, verbose=True):\n    \"\"\"`preprocess_raw_dataframe`\n\n    Pre-process a raw `CLP`/`XLP` DataFrame by applying the next steps:\n\n    * Rename columns (according to the given data dictionary mapping)\n    * Cleaning of numerical and categorical columns\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        Raw DataFrame (as read using [`kipy.read_excel_file_raw`][])\n\n    product : str\n        The product of interest. It accepts the following values:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    path_dictionary : str, pathlib.Path\n        Location of the `.xlsx` data dictionary.\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n\n    Returns\n    -------\n    df_preprocessed : pandas DataFrame\n        Pre-processed DataFrame.\n\n    See also\n    ----------\n    * [`kipy.rename_columns`][]\n    * [`kipy.clean_dataframe`][]\n    \"\"\"    \n\n    # Since verbose is handled internally for each function, there's \n    # no need to add any extra prints here.\n    df_renamed = rename_columns(df, product, path_dictionary, verbose=verbose)   \n    df_preprocessed = clean_dataframe(df_renamed, product, path_dictionary, verbose=verbose)\n\n    return df_preprocessed\n</code></pre>"},{"location":"api_reference/preprocessing/#kipy.clean_dataframe--see-also","title":"See also","text":"<ul> <li><code>kipy.clean_column_numerical</code></li> <li><code>kipy.clean_column_categorical</code></li> </ul> Source code in <code>kipy\\preprocessing.py</code> <pre><code>def clean_dataframe(df, product, path_dictionary, cols=None, verbose=True):\n    \"\"\"`clean_dataframe`\n\n    Clean categorical and numerical columns of a `CLP`/`XLP` DataFrame.\n\n    !!! info\n        To identify them, it uses information from the given data dictionary.\n        Numerical columns are those that have a `Type` of `int`, `float`, or\n        `int (scientific notation)`. Categorical columns are those that have a \n        `Type` of `string`. Columns that fall outside of these types remain\n        unchanged.\n\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        The pandas DataFrame.\n\n    product : str\n        The product of interest. It accepts the following values:\n\n        * `yescarta` or `clp`\n        * `tecartus` or `xlp`\n\n    path_dictionary : str, pathlib.Path\n        Location of the `.xlsx` data dictionary.\n\n    cols : list of str\n        List with the columns to be cleaned. If `None`, \n        all columns will be (attempted to be) cleaned.\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    df_clean : pandas DataFrame\n        Clean DataFrame.\n\n    See also\n    ----------\n    * [`kipy.clean_column_numerical`][]\n    * [`kipy.clean_column_categorical`][]\n    \"\"\"\n\n    if verbose:\n        print(\"Cleaning columns...\")\n\n    # Make sure that the given path is a pathlib.Path.\n    path_dictionary = pathlib.Path(path_dictionary)\n\n    # Check that data dictionary exists.\n    if not path_dictionary.exists():\n        raise Exception(f\"Data dictionary {path_dictionary} does not exist.\")\n\n    # Read the dictionary.\n    df_dictionary = pd.read_excel(path_dictionary, \n                                  sheet_name='dictionary', \n                                  header=0)\n\n    # Select the columns of interest.\n    df_dictionary = df_dictionary[['Computer Column Name', 'Unit', 'Type']]\n    df_dictionary = df_dictionary.set_index('Computer Column Name')\n\n\n    # Define which columns will be cleaned.\n    if cols is None:\n        cols = df.columns\n\n    # Perform cleaning of columns.\n    # This is done one by one and depending on the column type.\n    types_numerical = ['int', 'float', 'int (scientific notation)']\n    types_categorical = ['string']\n\n    df_clean = df.copy()\n    for col in cols:\n        col_unit = str(df_dictionary.loc[col, 'Unit']).lower()\n        col_type = str(df_dictionary.loc[col, 'Type']).lower()\n\n        if col_type in types_numerical:\n            if verbose:\n                print(f\"+ Cleaning numerical column {col}...\", end='', flush=True)\n            df_clean[col] = clean_column_numerical(df, col)\n\n            if col_unit == '%':\n                df_clean[col] = df_clean[col] * 100\n\n            if verbose:\n                print(\"\\t DONE!\")\n\n        elif col_type in types_categorical:\n            if verbose:\n                print(f\"+ Cleaning categorical column {col}...\", end='', flush=True)\n            df_clean[col] = clean_column_categorical(df, col)\n            if verbose:\n                print(\"\\t DONE!\")\n        else:\n            if verbose:\n                print(f\"- Column {col} will not be cleaned and left as is.\")\n\n\n    if verbose:\n        print(\"\\tDONE!\")\n\n\n    return df_clean\n</code></pre>"},{"location":"api_reference/preprocessing/#kipy.clean_column_numerical--see-also","title":"See also","text":"<ul> <li><code>kipy.clean_dataframe</code></li> </ul> Source code in <code>kipy\\preprocessing.py</code> <pre><code>def clean_column_numerical(df, col):\n    \"\"\"`clean_column_numerical`\n\n    Clean a numerical column. It applies the following steps:\n\n    * Convert empty spaces (i.e., `' '`) to `NaN`s.\n    * Convert entries with a value of `'No Result'` to `NaN`s.\n    * Convert weird entries with a value of `\\\\xa0` to `NaN`s.\n    * Convert entries with a value of `'nan'` to `NaN`s.\n    * Convert `LOQ`s (i.e., entries that have a `&lt;` such as `&lt;LOQ` or `&lt;8`) \n      to `-1`.\n    * Convert entries with a value of `#VALUE!` or `#DIV/0!` to `NaN`s.\n    * Remove the percentage character.\n    * Cast to float to ensure that values will be numbers.\n\n    !!! warning\n\n        Notice that percentage entries are cleaned by removing the `%` only.\n        For example, `75%` is cleaned as `0.75`. If an output of `75` is\n        needed, it needs to be multiplied by `100` outside of this function\n        (for example, as done in [`kipy.clean_dataframe`][])\n\n    !!! danger\n\n        If the `LOQ` definition changes in the future, this function needs \n        to be updated.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        The pandas DataFrame.s\n\n    col : string\n        Name of the numerical column to be cleaned.\n\n    Returns\n    -------\n    col_clean: pandas Series\n        The clean (numerical) column.\n\n    See also\n    ----------\n    * [`kipy.clean_dataframe`][]\n    \"\"\"\n    df_clean = df.copy(deep=True)\n\n    # Clean weird string entries.\n    df_clean.loc[df[col]==' ', col] = np.nan\n    df_clean.loc[df[col]=='No Result', col] = np.nan\n    df_clean.loc[df[col]=='\\xa0', col] = np.nan\n    df_clean.loc[df[col]=='nan', col] = np.nan\n\n    # Deal with LOQs\n    df_clean.loc[df[col]=='&lt;LOQ', col] = -1\n    df_clean.loc[df[col]=='&lt;10%', col] = -0.01 # Since this is a % column, it will be multiplied by 100.\n    df_clean.loc[df_clean[col].astype(str).str.contains('&lt;'), col] = -1\n\n    # In case we want to clean LOQs diffrently (e.g., &lt;10 --&gt; 10)\n    # df_clean[col] = df_clean[col].astype(str).str.replace('&lt;', '')\n\n\n    # Deal with wrong (Excel) formula results.\n    df_clean.loc[df[col]=='#VALUE!', col] = np.nan\n    df_clean.loc[df[col]=='#DIV/0!', col] = np.nan\n\n\n    # Remove percentage character.\n    df_clean[col] = df_clean[col].astype(str).str.replace('%', '')\n\n    # Cast to float (i.e., ensure that it will be a number).\n    df_clean[col] = df_clean[col].astype(float)\n\n    return df_clean[col]\n</code></pre>"},{"location":"api_reference/preprocessing/#kipy.clean_column_categorical--see-also","title":"See also","text":"<ul> <li><code>kipy.clean_dataframe</code></li> </ul> Source code in <code>kipy\\preprocessing.py</code> <pre><code>def clean_column_categorical(df, col):\n    \"\"\"`clean_column_categorical`\n\n    Clean a categorical column. It applies the following steps:\n\n    * Make strings lower case\n    * Remove leading spaces\n    * Remove trailing spaces\n    * Convert weird entries with a value of `\\\\xa0` to `NaN`s.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        The pandas DataFrame.\n\n    col : string\n        Name of the numerical column to be cleaned.\n\n    Returns\n    -------\n    col_clean: pandas Series\n        The clean (numerical) column.\n\n    See also\n    ----------\n    * [`kipy.clean_dataframe`][]\n    \"\"\"\n\n    df_clean = df.copy(deep=True)\n\n    # Make lower case and remove leading/trailing spaces.\n    def _clean_string(string):\n\n        if isinstance(string, str):\n            clean_string = string.lower().strip()\n        else:\n            clean_string = string\n\n        return clean_string\n    df_clean[col] = df_clean[col].apply(_clean_string)\n\n    # Remove weird string entries.\n    df_clean.loc[df[col]=='\\xa0', col] = np.nan\n\n    return df_clean[col]\n</code></pre>"},{"location":"api_reference/visualizations/","title":"Visualizations","text":"<p>These functions allow generating plots that are often required.</p> <p><code>plot_bar</code></p> <p>Plot a bar chart of the column of interest either in absolute or relative numbers.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame to be plotted.</p> required <code>col</code> <code>str</code> <p>Column of the DataFrame to be plotted.</p> required <code>style</code> <code>str</code> <p>String that defines the style. Possible values are:</p> <ul> <li><code>'absolute'</code> or <code>'n'</code> - Number of batches</li> <li><code>'relative'</code> or <code>'p'</code> - Percentage of batches</li> </ul> <code>'absolute'</code> <code>order</code> <code>list of str</code> <p>Order to plot. If <code>None</code>, order is inferred from the data.</p> <code>None</code> <code>want_num_labels</code> <code>bool</code> <p>Boolean that defines if bars will have number labels (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <code>fig</code> <code>matplotlib figure handle</code> <p>Figure handle.</p> <code>None</code> <code>ax</code> <code>matplotlib ax handle</code> <p>Axes handle.</p> <code>None</code> <code>palette</code> <code>str</code> <p>Any string that can be interpreted by seaborn <code>color_palette</code> (including any of <code>matplotlib</code>'s color maps'). Some examples are <code>'viridis'</code>, <code>'winter'</code>, <code>'jet'</code>. </p> <p>Tip</p> <p>Notice you can reverse any palette by appending <code>'_r'</code> at the end  of the name (e.g., <code>'viridis_r'</code>).</p> <code>'viridis'</code> <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Type Description <code>fig, ax</code> <p>For further customization outside of the function.</p> <p> </p> <p><code>plot_bar_by_date</code></p> <p>Plot a bar chart of the column of interest ordered by date either in absolute or relative numbers.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame to be plotted.</p> required <code>col</code> <code>str</code> <p>Column of the DataFrame to be plotted.</p> required <code>col_date</code> <code>str</code> <p>Column of the DataFrame with date information.</p> required <code>style</code> <code>str</code> <p>String that defines the style. Possible values are:</p> <ul> <li><code>'absolute'</code> or <code>'n'</code> - Number of batches</li> <li><code>'relative'</code> or <code>'p'</code> - Percentage of batches</li> </ul> <code>'absolute'</code> <code>want_num_labels</code> <code>bool</code> <p>Boolean that defines if bars will have number labels (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <code>fig</code> <code>matplotlib figure handle</code> <p>Figure handle.</p> <code>None</code> <code>ax</code> <code>matplotlib ax handle</code> <p>Axes handle.</p> <code>None</code> <code>palette</code> <code>str</code> <p>Any string that can be interpreted by seaborn <code>color_palette</code> (including any of <code>matplotlib</code>'s color maps'). Some examples are <code>'viridis'</code>, <code>'winter'</code>, <code>'jet'</code>. </p> <p>Tip</p> <p>Notice you can reverse any palette by appending <code>'_r'</code> at the end  of the name (e.g., <code>'viridis_r'</code>).</p> <code>'viridis'</code> <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Type Description <code>fig, ax</code> <p>For further customization outside of the function.</p> <p> </p> <p><code>plot_distribution</code></p> <p>Plot the distribution of a DataFrame column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame to be plotted.</p> required <code>col</code> <code>str</code> <p>Column name of <code>df</code></p> required <code>want_metrics</code> <code>bool</code> <ul> <li>If <code>True</code>, basic descriptive metrics (mean, STD, min, max,  percentiles) are displayed in the plot (as a legend). </li> <li>If <code>False</code>, none are displayed.</li> </ul> <code>False</code> <code>tolerance_int</code> <code>int or str</code> <p>Coverage of the tolerance intervals defined as follows:</p> <ul> <li><code>67</code> or <code>'67'</code>: 1 STD, thus covering 67% of the data (assuming normality)</li> <li><code>95</code> or <code>'95'</code>: 2 STD, thus covering 95% of the data (assuming normality)</li> <li><code>99</code> or <code>'99'</code>: 3 STD, thus covering 99% of the data (assuming normality)</li> <li><code>0</code> or <code>'0'</code> or <code>None</code>: Tolerance intervals are not plotted.</li> </ul> <code>None</code> <code>fig</code> <code>matplotlib figure handle</code> <p>Figure handle.</p> <code>None</code> <code>ax</code> <code>matplotlib ax handle</code> <p>Axes handle.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Type Description <code>fig, ax</code> <p>For further customization outside of the function.</p> <p> </p> <p><code>plot_distribution_box</code></p> <p>Plot the distribution of a DataFrame column with a boxplot on top.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame to be plotted.</p> required <code>col</code> <code>str</code> <p>Column name of interest present in <code>df</code></p> required <code>want_metrics</code> <code>bool</code> <ul> <li>If <code>True</code>, basic descriptive metrics (mean, STD, min, max,  percentiles) are displayed in the plot (as a legend). </li> <li>If <code>False</code>, none are displayed.</li> </ul> <code>False</code> <code>tolerance_int</code> <code>int, str</code> <p>Coverage of the tolerance intervals defined as follows:</p> <ul> <li><code>67</code> or <code>'67'</code>: 1 STD, thus covering 67% of the data (assuming normality)</li> <li><code>95</code> or <code>'95'</code>: 2 STD, thus covering 95% of the data (assuming normality)</li> <li><code>99</code> or <code>'99'</code>: 3 STD, thus covering 99% of the data (assuming normality)</li> <li><code>0</code> or <code>'0'</code> or <code>None</code>: Tolerance intervals are not plotted.</li> </ul> <code>None</code> <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>matplotlib figure handle</code> <p>Figure handle</p> <p>[ax_box, ax_dist] : a list     List of plot axes</p> <p> </p> <p><code>plot_missing_values</code></p> <p>Create visualization of missing values in a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pandas DataFrame</code> <p>DataFrame</p> required <code>col_sort</code> <code>str</code> <p>Column to sort <code>df</code>'s rows. If <code>None</code>, no sorting is performed.</p> <code>None</code> <code>fig</code> <code>matplotlib figure handle</code> <p>Figure handle.</p> <code>None</code> <code>ax</code> <code>ax handle</code> <p>Axes handle.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Define if verbose output will be printed (<code>True</code>) or not (<code>False</code>).</p> <code>True</code> <p>Returns:</p> Type Description <code>fig, ax</code> <p>For further customization outside of the function.</p> <p></p>"},{"location":"api_reference/visualizations/#kipy.plot_bar--displays","title":"Displays","text":"<p>Bar chart.</p>"},{"location":"api_reference/visualizations/#kipy.plot_bar--see-also","title":"See also","text":"<ul> <li>See <code>kipy.plot_bar_by_date</code> for a bar plot with a time dimension.</li> </ul> Source code in <code>kipy\\visualizations.py</code> <pre><code>def plot_bar(df, col, style='absolute', order=None, want_num_labels=True, fig=None, ax=None, palette='viridis', verbose=True):\n    \"\"\"`plot_bar`\n\n    Plot a bar chart of the column of interest either in absolute or relative numbers.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be plotted.\n\n    col : str\n        Column of the DataFrame to be plotted.\n\n    style : str\n        String that defines the style. Possible values are:\n\n        * `'absolute'` or `'n'` - Number of batches\n        * `'relative'` or `'p'` - Percentage of batches\n\n    order : list of str\n        Order to plot. If `None`, order is inferred from the data.\n\n    want_num_labels : bool\n        Boolean that defines if bars will have number labels (`True`)\n        or not (`False`).\n\n    fig : matplotlib figure handle\n        Figure handle.\n\n    ax : matplotlib ax handle\n        Axes handle.\n\n    palette : str\n        Any string that can be interpreted by seaborn [`color_palette`](https://seaborn.pydata.org/generated/seaborn.color_palette.html#seaborn.color_palette)\n        (including any of [`matplotlib`'s color maps'](https://matplotlib.org/stable/users/explain/colors/colormaps.html)).\n        Some examples are `'viridis'`, `'winter'`, `'jet'`. \n\n        !!! tip \n            Notice you can reverse any palette by appending `'_r'` at the end \n            of the name (e.g., `'viridis_r'`).\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    fig, ax : tuple\n        For further customization outside of the function.\n\n    Displays\n    --------\n    Bar chart.\n\n    See also\n    ----------\n    * See [`kipy.plot_bar_by_date`][] for a bar plot with a time dimension.\n    \"\"\"\n    if verbose:\n        print(f\"Generating bar chart of {col}...\", end='', flush=False)\n\n    # Create figure (if necessary).\n    if (fig is None) and (ax is None):\n        fig, ax = plt.subplots(1, 1, figsize=[8, 6])\n    elif fig is None:\n        fig = ax.get_figure()\n    elif ax is None:\n        ax = fig.gca()\n\n    df_plot = df.copy()\n\n    # Check that columns are in the given DataFrame.\n    if col not in df_plot.columns:\n        raise Exception(f\"Column {col} not present in DataFrame\")\n\n    # Actually plot.\n    if (style=='absolute') or (style=='n'):\n        sns.countplot(x=df_plot[col], ax=ax, palette='viridis')\n\n    elif (style=='relative') or (style=='p'):        \n        df_p = df[col].value_counts(normalize=True).reset_index().rename({'index':col, col:'percent'}, axis=1)\n        sns.barplot(x=col, y='percent', data=df_p, palette='viridis')\n\n\n    else:\n        raise Exception(f\"{style} is not a valid plot style.\")\n\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n    # Add counts to each bar.\n    # See https://stackoverflow.com/a/68334380/948768\n    # and https://stackoverflow.com/a/76241039/948768\n    if want_num_labels:\n        if (style=='absolute') or (style=='n'):\n            ax.bar_label(ax.containers[0])\n        elif (style=='relative') or (style=='p'):\n            labels = [f'{(h)*100:0.1f}%' if (h := v.get_height()) &gt; 0 else '' for v in ax.containers[0]]\n            ax.bar_label(ax.containers[0], labels=labels)\n\n\n    ax.set_xlabel(col.replace('_', ' ').capitalize())\n\n    # Format x-axis tick labels so that x-ticks are integers.\n    # See https://stackoverflow.com/a/50045907/948768\n    if col in ['harvest_day', 'number_bags']:\n        xlabels = [item.get_text() for item in ax.get_xticklabels()]\n        ax.set_xticklabels([str(round(float(label))) for label in xlabels])\n\n    # Format y-axis tick labels so that y-ticks correspond to 0-100%.\n    if (style=='relative') or (style=='p'):\n        ylabels = [item.get_text() for item in ax.get_yticklabels()]\n        ax.set_yticks(ax.get_yticks().tolist()) # To avoid warning. See https://stackoverflow.com/a/68794383/948768\n        ax.set_yticklabels([str(round(float(label)*100)) for label in ylabels])\n\n    if (style=='absolute') or (style=='n'):\n        ax.set_ylabel(\"Number of Batches\")\n    elif (style=='relative') or (style=='p'):\n        ax.set_ylabel(\"Proportion of Batches [%]\")\n\n    if verbose:\n        print(\"\\tDONE!\")\n\n    return fig, ax\n</code></pre>"},{"location":"api_reference/visualizations/#kipy.plot_bar_by_date--displays","title":"Displays","text":"<p>Bar chart with time dimension.</p>"},{"location":"api_reference/visualizations/#kipy.plot_bar_by_date--see-also","title":"See also","text":"<ul> <li>See <code>kipy.plot_bar</code> for a bar plot of all batches (with no time dimension).</li> </ul> Source code in <code>kipy\\visualizations.py</code> <pre><code>def plot_bar_by_date(df, col, col_date, style='absolute', want_num_labels=True, fig=None, ax=None, palette='viridis', verbose=True):\n    \"\"\"`plot_bar_by_date`\n\n    Plot a bar chart of the column of interest ordered by date either in absolute or relative numbers.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be plotted.\n\n    col : str\n        Column of the DataFrame to be plotted.\n\n    col_date : str\n        Column of the DataFrame with date information. \n\n    style : str\n        String that defines the style. Possible values are:\n\n        * `'absolute'` or `'n'` - Number of batches\n        * `'relative'` or `'p'` - Percentage of batches\n\n    want_num_labels : bool\n        Boolean that defines if bars will have number labels (`True`)\n        or not (`False`).\n\n    fig : matplotlib figure handle\n        Figure handle.\n\n    ax : matplotlib ax handle\n        Axes handle.\n\n    palette : str\n        Any string that can be interpreted by seaborn [`color_palette`](https://seaborn.pydata.org/generated/seaborn.color_palette.html#seaborn.color_palette)\n        (including any of [`matplotlib`'s color maps'](https://matplotlib.org/stable/users/explain/colors/colormaps.html)).\n        Some examples are `'viridis'`, `'winter'`, `'jet'`. \n\n        !!! tip \n            Notice you can reverse any palette by appending `'_r'` at the end \n            of the name (e.g., `'viridis_r'`).\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    fig, ax : tuple\n        For further customization outside of the function.\n\n    Displays\n    --------\n    Bar chart with time dimension.\n\n    See also\n    ----------\n    * See [`kipy.plot_bar`][] for a bar plot of all batches (with no time dimension).\n    \"\"\"\n    if verbose:\n        print(f\"Generating bar chart (by date) of {col}...\", end='', flush=False)\n\n    # Create figure (if necessary).\n    if (fig is None) and (ax is None):\n        fig, ax = plt.subplots(1, 1, figsize=[8, 6])\n    elif fig is None:\n        fig = ax.get_figure()\n    elif ax is None:\n        ax = fig.gca()\n\n\n    # Check that columns are in the given DataFrame.\n    for col_ in [col, col_date]:\n        if col_ not in df.columns:\n            raise Exception(f\"Column {col_} not present in DataFrame\")\n\n\n    if (style=='absolute') or (style=='n'):\n        df_grouped = df.groupby(col_date)[col].value_counts(normalize=False).unstack(col)\n    elif (style=='relative') or (style=='p'):\n        df_grouped = df.groupby(col_date)[col].value_counts(normalize=True).unstack(col)\n    else:\n        raise Exception(f\"{style} is not a valid plot style.\")\n\n    df_grouped.plot.bar(stacked=True, cmap=cm.get_cmap(palette), width=0.8, ax=ax)\n\n\n    # Format x-axis tick labels\n    xlabels = ax.get_xticklabels()\n    ax.set_xticklabels(xlabels, rotation=25, ha='right', rotation_mode='anchor')\n\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.legend(bbox_to_anchor=(1.04, 0.95), title=col.replace('_', ' ').capitalize(), loc=\"upper left\", frameon=False)\n\n    # Format legend (make sure it displays integers)\n    leg = ax.get_legend()\n    for lbl in leg.get_texts():\n        label_text = lbl.get_text()\n        new_text = f\"{float(label_text):.0f}\"\n        lbl.set_text(new_text)\n\n\n    # Format y-axis tick labels so that y-ticks correspond to 0-100%.\n    if (style=='relative') or (style=='p'):\n        ylabels = [item.get_text() for item in ax.get_yticklabels()]\n        ax.set_yticks(ax.get_yticks().tolist()) # To avoid warning. See https://stackoverflow.com/a/68794383/948768\n        ax.set_yticklabels([str(round(float(label)*100)) for label in ylabels])\n\n        ax.set_ylim([0, 1.05])\n\n    ax.set_xlabel(\"Date\")\n\n    if (style=='absolute') or (style=='n'):\n        ax.set_ylabel(\"Number of Batches\")\n    elif (style=='relative') or (style=='p'):\n        ax.set_ylabel(\"Proportion of Batches [%]\")\n\n    # Add numeric labels.\n    if want_num_labels:\n        for idx, row in df_grouped.reset_index(drop=True).iterrows():\n            cumulative = 0\n\n            for element in row:\n                # Check to avoid NaNs\n                if element == element:\n\n                    if (style=='absolute') or (style=='n'):\n                        ax.text(idx, cumulative + element / 2,\n                            f\"{element:.0f}\",\n                            va='center',\n                            ha='center')\n                    elif (style=='relative') or (style=='p'):\n                        ax.text(idx, cumulative + element / 2,\n                            f\"{element * 100:.1f}%\",\n                            va='center',\n                            ha='center')\n                    cumulative += element\n\n    if verbose:\n        print(\"\\tDONE!\")\n\n    return fig, ax\n</code></pre>"},{"location":"api_reference/visualizations/#kipy.plot_distribution--displays","title":"Displays","text":"<p>Distribution plot.</p> Source code in <code>kipy\\visualizations.py</code> <pre><code>def plot_distribution(df, col, want_metrics=False, tolerance_int=None, fig=None, ax=None, verbose=True):\n    \"\"\"`plot_distribution`\n\n    Plot the distribution of a DataFrame column.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be plotted.\n\n    col : str\n        Column name of `df` \n\n    want_metrics : bool\n        * If `True`, basic descriptive metrics (mean, STD, min, max, \n        percentiles) are displayed in the plot (as a legend). \n        * If `False`, none are displayed.\n\n    tolerance_int : int or str\n        Coverage of the tolerance intervals defined as follows:\n\n        * `67` or `'67'`: 1 STD, thus covering 67% of the data (assuming normality)\n        * `95` or `'95'`: 2 STD, thus covering 95% of the data (assuming normality)\n        * `99` or `'99'`: 3 STD, thus covering 99% of the data (assuming normality)\n        * `0` or `'0'` or `None`: Tolerance intervals are not plotted.\n\n    fig : matplotlib figure handle\n        Figure handle.\n\n    ax : matplotlib ax handle\n        Axes handle.\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    fig, ax : tuple\n        For further customization outside of the function.\n\n    Displays\n    --------\n    Distribution plot.\n    \"\"\"\n\n    if verbose:\n        print(f\"Generating distribution plot of column {col}...\", end='', flush=False)\n\n    # Create figure (if necessary).\n    if (fig is None) and (ax is None):\n        fig, ax = plt.subplots(1, 1, figsize=[8, 6])\n    elif fig is None:\n        fig = ax.get_figure()\n    elif ax is None:\n        ax = fig.gca()\n\n\n    # Check that column is in the given DataFrame.\n    if col not in df.columns:\n        raise Exception(f\"Column {col} not present in DataFrame\")\n\n\n    # Create basic plot.\n    sns.histplot(data=df, x=col, kde=True, ax=ax)\n\n    # Get the original xlim (to set it again later)\n    xlim = ax.get_xlim()\n\n    # Descriptive statistics.\n    # In order to show the descriptive metrics in the plot, we will\n    # \"plot\" them as invisible lines and add them to the legend.\n    if want_metrics:\n        metrics = df[col].describe()\n\n        for param, value in metrics.items():\n\n            # We skip count, since it isn't interesting (and plotting it\n            # makes no sense).\n            if param == 'count':\n                continue\n\n            ax.axvline(value, color=[1, 1, 1], alpha=0, linestyle='--', label=f\"{param.capitalize()} = {value:.2f}\")\n\n\n    # Plot tolerance interval.\n    # Now, we define it as the mean +/- 2STD\n    allowed_tolerances = [68, '68', 95, '95', 99, '99']\n    no_tolerances = [0, '0', None]\n    if tolerance_int in allowed_tolerances:\n        metrics = df[col].describe()\n        mean = metrics.loc['mean']\n        std = metrics.loc['std']\n\n        # Indicate the number of STD to include in the tolerance intervals\n        if (tolerance_int == 68) or (tolerance_int == '68'):\n            n_std = 1\n            tolerance_str = '68'\n        if (tolerance_int == 95) or (tolerance_int == '95'):\n            n_std = 2\n            tolerance_str = '95'\n        if (tolerance_int == 99) or (tolerance_int == '99'):\n            n_std = 3\n            tolerance_str = '99'\n\n        tolerance_int = [mean - (n_std*std), mean + (n_std*std)]\n        ax.axvspan(xmin=tolerance_int[0], xmax=tolerance_int[1], color='g', alpha=0.1, label=f\"TI ({tolerance_str}%) = {tolerance_int[0]:.2f} - {tolerance_int[1]:.2f}\")\n\n\n    elif tolerance_int in no_tolerances:\n        pass\n    else:\n        print(\"\\nInvalid tolerance interval. Tolerance intervals will not be plotted.\")\n\n\n    # Show the legend.\n    if want_metrics or tolerance_int:\n        ax.legend(bbox_to_anchor=(1.04, 0.95), title=col.replace('_', ' ').capitalize(), loc='upper left', frameon=False)\n\n    # Set xlim back to the original\n    # Since we \"plot\" the lines of the metrics (just so that they appear\n    # on the legend), we need to switch back the x-lim to the original.\n    ax.set_xlim(xlim)\n\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n\n    ax.set_xlabel(col.replace('_', ' ').capitalize())\n    ax.set_ylabel(\"No. batches\")\n\n    # plt.show()\n\n    if verbose:\n        print(\"\\tDONE!\")\n\n    return fig, ax\n</code></pre>"},{"location":"api_reference/visualizations/#kipy.plot_distribution_box--displays","title":"Displays","text":"<p>Distribution plot with boxplot on top.</p> Source code in <code>kipy\\visualizations.py</code> <pre><code>def plot_distribution_box(df, col, want_metrics=False, tolerance_int=None, verbose=True):\n    \"\"\"`plot_distribution_box`\n\n    Plot the distribution of a DataFrame column with a boxplot on top.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame to be plotted.\n\n    col : str\n        Column name of interest present in `df` \n\n    want_metrics : bool\n        * If `True`, basic descriptive metrics (mean, STD, min, max, \n        percentiles) are displayed in the plot (as a legend). \n        * If `False`, none are displayed.\n\n    tolerance_int : int, str\n        Coverage of the tolerance intervals defined as follows:\n\n        * `67` or `'67'`: 1 STD, thus covering 67% of the data (assuming normality)\n        * `95` or `'95'`: 2 STD, thus covering 95% of the data (assuming normality)\n        * `99` or `'99'`: 3 STD, thus covering 99% of the data (assuming normality)\n        * `0` or `'0'` or `None`: Tolerance intervals are not plotted.\n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    fig : matplotlib figure handle\n        Figure handle\n\n    [ax_box, ax_dist] : a list\n        List of plot axes\n\n    Displays\n    --------\n    Distribution plot with boxplot on top.\n    \"\"\"\n\n    if verbose:\n        print(f\"Generating distribution plot + boxplot of column {col}...\", end='', flush=False)\n\n    fig, (ax_box, ax_dist) = plt.subplots(2, 1, figsize=[8, 6], sharex=True, gridspec_kw={'height_ratios': (0.15, 0.85)})\n\n    sns.boxplot(data=df, x=col, ax=ax_box)\n    ax_box.set_xlabel('')\n    ax_box.spines['top'].set_visible(False)\n    ax_box.spines['right'].set_visible(False)\n    ax_box.spines['left'].set_visible(False)\n\n    plot_distribution(df, col, want_metrics=want_metrics, tolerance_int=tolerance_int, fig=None, ax=ax_dist, verbose=False)\n\n    fig.tight_layout(h_pad=3)\n\n    # plt.show()\n\n    if verbose:\n        print(\"\\tDONE!\")\n\n    return fig, [ax_box, ax_dist]\n</code></pre>"},{"location":"api_reference/visualizations/#kipy.plot_missing_values--displays","title":"Displays","text":"<p>Visualization of missing values.</p> Source code in <code>kipy\\visualizations.py</code> <pre><code>def plot_missing_values(df, col_sort=None, fig=None, ax=None, verbose=True):\n    \"\"\"`plot_missing_values`\n\n    Create visualization of missing values in a DataFrame.\n\n    Parameters\n    ----------\n    df : pandas DataFrame\n        DataFrame\n\n    col_sort : str\n        Column to sort `df`'s rows. If `None`, no sorting is performed.\n\n    fig : matplotlib figure handle\n        Figure handle.\n\n    ax : ax handle\n        Axes handle.   \n\n    verbose : bool\n        Define if verbose output will be printed (`True`) or not (`False`).\n\n    Returns\n    -------\n    fig, ax : tuple\n        For further customization outside of the function.\n\n    Displays\n    --------\n    Visualization of missing values.\n    \"\"\"      \n\n    if verbose:\n        print(\"Generating plot of missing values...\", end='', flush=False)\n\n    # Create figure (if necessary).\n    if (fig is None) and (ax is None):\n        fig, ax = plt.subplots(1, 1, figsize=[20, 14])\n    elif fig is None:\n        fig = ax.get_figure()\n    elif ax is None:\n        ax = fig.gca()\n\n\n    if col_sort is not None:\n        if col_sort in df.columns:\n            df = df.sort_values(by=[col_sort])\n            if verbose:\n                print(f\"\\n\\tDataFrame was sorted by column {col_sort}\")\n        else:\n            print(f\"\\n\\tColumn {col_sort} was not found in df. No sorting will be performed.\")\n\n\n    # Plot.\n    fontsize = 20\n    msno.matrix(df=df, ax=ax, color=(0.2, 0.2, 0.2), sparkline=False, fontsize=fontsize)\n\n    # plt.show()\n\n    if verbose:\n        print(\"\\tDONE!\")\n\n    return fig, ax\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>We have created a few Jupyter notebooks where we showcase the use of <code>KiPy</code>.  These are hosted in <code>KiPy</code>'s GitHub repository and can be found here:</p> <ol> <li>Basics</li> <li>Visualizations</li> <li>Process monitoring - This notebook is hosted externally and is part of the <code>process-monitoring</code> repository.</li> </ol> <p>Please note that to access them, you need a GitHub account (using your Gilead email) and proper rights. If you haven't been added to MSAT's GitHub teams, please reach out to arturo.moncadatorres@gilead.com</p>"}]}